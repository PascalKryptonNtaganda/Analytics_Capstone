####################################################################
# 4.2.4.1.3 PCA For Cluster 0
####################################################################

pca_prep = PCA().fit(X_C0s_2022)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 26  
n_pc = 26
pca_C02022 = PCA(n_components = n_pc).fit(X_C0s_2022)
Xp_C02022 = pca_C02022.transform(X_C0s_2022)
print(f'After PCA, we use {pca_C02022.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train_C02022, Xp_test_C02022, yp_train_C02022, yp_test_C02022 = train_test_split(Xp_C02022, y_C02022, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C02022 = RandomForestRegressorModel(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# Grid Seach for best Random Forest Regressor
FindRandomForestRegressorParam_GridSearch(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# For Gradient Boosting 
gbr_C02022 = GradientBoostingRegressorModel(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# Grid Seach for best Gradient Boosting
FindGradientBoostingRegressorParam_GridSearch(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

####################################################################
# 4.2.4.2 Analyzing Cluster 1
####################################################################

X_C12022 = c1_2022.loc[:, c1_2022.columns != 'Sale Price']
y_C12022 = c1_2022[['Sale Price']].values.ravel()
fn_C12022 = X_C12022.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C12022)
X_C1s_2022 = scaler.transform(X_C12022)


# Divide the dataset into training and testing data.
# Divide the scaled dataset into training and testing data
X_train_C1s_2022, X_test_C1s_2022, y_train_C12022, y_test_C12022 = train_test_split(X_C1s_2022, y_C12022, test_size =.30,random_state=1234) 

# Divide the dataset into training and testing data.
X_train_C12022, X_test_C12022, y_train_C12022, y_test_C12022 = train_test_split(X_C12022, y_C12022, test_size =.30,random_state=1234) 

####################################################################
# 4.2.4.2.1  Building Models With Default Parameters for Cluster 1
####################################################################

# Decision Tree Regressor
dtr_C12022 = DecisionTreeRegressorModel(X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
# Random Forest Regressor
rfr_C12022 = RandomForestRegressorModel(X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
# Gradient Boosting Regression
grbr_C12022 = GradientBoostingRegressorModel(X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)

# SVM Regression
svmr_C12022 = SVMRegressorModel('linear', 10, 0.01, X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)

# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2022 = FindNeuralNetworkParam_GridSearch(X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)
results_gs = pd.DataFrame(gridSearch_2022.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)


####################################################################
# 4.2.4.2.2 Feature Selection for cluster 1
####################################################################

####################################################################
# 4.2.4.2.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C12022.feature_importances_
np.sum(importances)
plt.barh(fn_C12022, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C12022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C1_reduced = selector.fit_transform(X_C12022, y_C12022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C12022):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C12022, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train))

####################################################################
# 4.2.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C12022.feature_importances_
np.sum(importances)
plt.barh(fn_C12022, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C12022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C1_reduced = selector.fit_transform(X_C12022, y_C12022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C12022):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C12022, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

####################################################################
# 4.2.4.2.3 PCA For Cluster 1
####################################################################

pca_prep = PCA().fit(X_C1s_2022)
pca_prep.n_components_
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 24  
n_pc = 24
pca_C12022 = PCA(n_components = n_pc).fit(X_C1s_2022)
Xp_C12022 = pca_C12022.transform(X_C1s_2022)
print(f'After PCA, we use {pca_C12022.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train_C12022, Xp_test_C12022, yp_train_C12022, yp_test_C12022 = train_test_split(Xp_C12022, y_C12022, test_size =.2, random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C12022 = RandomForestRegressorModel(Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)
# For Gradient Boosting 
gbr_C12022 = GradientBoostingRegressorModel(Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)      

