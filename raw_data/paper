
Factors Influencing Residential Properties Sales Price Pre- Covid and Post-Covid and A Machine Learning Approach on Predicting Market Value in 2023 for Residential Properties


MANSI MAHAJAN, PASCAL NTAGANDA, and JIBOK PARK
BUAN 5510
Dr. Ben Kim
Aug 11, 2023


 
Abstract	2
1.	Introduction	3
a.	Problem Statement	4
b.	Contribution	4
c.	Background Information:	4
2.	Description of Datasets and Data Exploration	5
a.	Description of Datasets	5
b.	Data Exploration	5
3.	Literature Reviews	7
4.	Data Pre-Processing	9
a.	Processing of NULL values	10
c.	Merging external datasets	11
e.	Use clustering for preprocessing	13
5.	Data Mining Models and Evaluations	14
a.	Evaluation Metrics:	14
b.	Statement1	15
c.	Statement2	19
6.	Discussion	22
7.	Conclusion	23
References	25
Appendices	27
Data Dictionary:	34
Code	34
 
Abstract
This study investigates the significant features influencing the sales price of residential properties both pre and post Covid-19, aiming to identify potential variations in factors affecting property values during these periods. In addition, we aim to build a machine learning model capable of predicting the total market value of residential properties.
The impact of Covid-19 on various industries, including real estate, has been substantial, and understanding the changes in property sales dynamics is crucial for making informed decisions in the current market. By comparing the influential features for residential property sales price pre and post Covid-19, we seek to uncover any shifts in market demand and preferences that may have occurred due to the pandemic.
To ensure a homogeneous subset of properties, the dataset is narrowed down to a specific time range, reducing the impact of outliers, and ensuring that the properties considered share similar characteristics and market conditions. This approach enhances the reliability and accuracy of our predictions.
Overall, this study seeks to contribute valuable insights into the factors influencing residential property sales prices both pre and post Covid-19, and to develop an effective machine learning model for predicting the total market value in 2023 for residential properties built in the last ten years. The findings may provide valuable guidance for real estate professionals, investors, and policymakers in navigating the dynamic real estate market and to make informed decisions.
 
1.	Introduction
a.	Problem Statement:
1)	What are the significant features impacting the Sales Price of Residential Properties pre Covid and post Covid?
2)	To build a machine learning model for predicting the Total Market Value in 2023 for Residential Properties built in the last 10 years.
b.	Contributions
This study makes several noteworthy contributions to the field of real estate analysis and data-driven decision-making. Firstly, by analyzing the factors influencing residential property sales prices both before and after the pandemic, our research provides valuable insights into the shifting landscape of buyer preferences and market trends brought about by the COVID-19 global health crisis. This enhanced understanding of the pandemic's impact on the real estate market aids in predicting and adapting to long-term changes in housing demand and pricing. Secondly, our implementation of a machine learning model for predicting market value of residential properties in 2023 highlights the application of advanced data analysis techniques to the real estate domain. This novel model facilitates accurate market value predictions based on historical data and property attributes, empowering stakeholders such as property owners, investors, and policymakers to make informed decisions for the upcoming tax year. Additionally, our focus on properties built within the last ten years provides specific insights into the behavior of newer properties in response to evolving market dynamics and buyer preferences. The practical implications of our research extend to real estate agents, developers, and investors, guiding them on how to adapt their strategies and offerings to cater to the changing needs and expectations of buyers in a post-pandemic environment. Overall, our study contributes significantly to the growing body of knowledge concerning the real estate market's response to significant global events and demonstrates the importance of employing machine learning techniques for data-driven predictions in the housing sector.
c.	Background Information: 
The real estate market has always been influenced by a wide variety of factors that influence property values. However, the COVID-19 pandemic's emergence changed the dynamics of the housing market. This research paper builds upon prior research and develops several machine learning models to understand the significant factors influencing Sales Price of Residential Houses in Pierce County, WA based on the properties characteristics as well as quality of local schools. In addition, the study aims to develop a predictive model to estimate the total market value in 2023 for Residential properties built in the Pierce County over the last ten years. The results of this study can offer valuable insights for homebuyers, sellers, real estate agents, and policymakers to the broader understanding of the real estate market dynamics and provide valuable insights for various stakeholders in the industry.

2. Description of Datasets and Data Exploration 
a.	Dataset Description		
We sourced property information data1) from the Pierce County Assessor-Treasurer’s Data website to facilitate our study. The dataset encompasses a collection of nine distinct tables, each offering a unique perspective on property-related attributes. For our research objectives, we have chosen to focus on five key datasets: Appraisal Account, Improvement, Improvement Built-as, Sale, and Tax Account. The Appraisal Account table had 6.0 MB of data, distributed across 24 attributes, and 338,546 individual data records. The Improvement table contributes an additional 3.8 MB of data, with 356,693 records and 25 attributes that show property improvements.
The Improvement Built-as table has a size of 4.3 MB, and 359,476 records intricately woven into 26 attributes. Subsequently, the Sale table, recognized as the biggest dataset, with 18.2 MB in size. This table has 598,670 rows of sale data spanning from 1997 to 2022, with 13 attributes. The Tax Account table, our final focal dataset had 8.5 MB in size and comprised of 350,472 rows of data. This table had 28 attributes. Address Point data sourced from Pierce County was also used. This integration enabled us to effectively merge property-related information with the school scores, thereby fostering a holistic understanding of the underlying relationships and effects.
b.	Data Exploration
Initially, we conducted a correlation analysis due to the potential adverse effects of multicollinearity, including unstable model coefficients, challenges in interpretation, and decreased model generalization. As a result, we identified pairs of attributes with correlations exceeding 0.7 and retained only one attribute from each pair. This process led to the removal of 6 attributes from the Appraisal, 2 from the Improvement, 4 from ImprovementBuiltAs, and 1 from Sale, as well as 22 attributes from the Tax Account. Additionally, non-essential string-type attributes like ID or code were excluded before dataset merging.
Upon merging the datasets, we observed that dataset for statement 1 contained 11,295 records for 2019 and 10247 for 2022 with 44 attributes related to sale price data, while statement 2 featured 17,246 and 24 attributes associated with tax data. Due to varying record counts in the initial tables, employing an outer join resulted in null values, prompting us to investigate further.
We tried to understand the trends within the dependent attributes, namely Sale Price and Market Value, and their correlations with other attributes. Notably, the mean Sale Price in 2022 exceeded that of 2019, accompanied by a wider price range. (Fig1) An intriguing insight emerged as house quality exhibited an exponential relationship with price, indicating that superior quality correlated with significantly higher prices. (Fig2)
Comparing transaction patterns between 2019 and 2022, we observed a shift in peak activity from the summer months to an earlier onset in March for the latter year, followed by a subdued summer. Despite a higher transaction volume in 2019, price fluctuations were more pronounced in 2022. Notably, the appreciation in house prices associated with increased square footage was more rapid in 2022 than in 2019, implying a preference for larger homes in the former year. Driven by these findings, our focus turns to a deeper exploration of how individual attributes exert distinct influences on house prices during the years 2019 and 2022.
Shifting our gaze to the taxable market value based on the year of construction, a conspicuous pattern emerges; houses built in recent years exhibit a distinct trend compared to those constructed before the early 2010s. This distinction prompts us to consider employing separate Machine Learning models for new and old houses, with our immediate attention centered on the dynamics of new houses.
Figure1. Sale Price by Quality in 2019 vs 2022            	Figure2. Market Value by Year Built
  
3.	 Literature Reviews
a.	Predictive analytics using Big Data for the real estate market during the COVID-19 pandemic 2)
The article aimed to identify apartment attributes influencing price revisions during the pandemic using SHAP values and exploring the impact of Time on the Market (TOM) on apartment prices. The three-step methodology involved data mining through web scraping, resulting in 18,992 data points with 16 variables. They then conducted data cleaning and preparation, rearranging variables for analysis and adding dummy variables. Employing 15 machine learning algorithms, the study found that apartment prices remained resilient during the pandemic, with TOM and initial price setup emerging as the most dominant and consistent variables influencing price revisions.
b.	Housing Price Prediction Using Machine Learning Algorithms in COVID-19 Times3)
This study pursued two main objectives: to identify optimal machine learning algorithms for predicting housing prices and to assess the pandemic's impact on housing prices in Alicante, Spain. Approximately 40,000 property samples from 2019 to 2021 were collected and analyzed using web scraping to understand COVID-19's influence on property prices. The data cleaning process involved adjusting variables causing multicollinearity and removing extreme data points. Boosting-based algorithms outperformed basic linear methods. Key predictors for housing prices were floor area, number of bathrooms, and elevator presence. Despite using data from different periods, regional characteristics of coastal cities played a decisive role. Compared to other regions, housing prices were lower, and net household income significantly influenced purchasing power and predicted housing prices. The pandemic's impact was localized and temporary, displaying housing market resilience. 
c.	Real estate price forecasting based on SVM optimized by PSO (Particle Swarm Optimization) 4)
The paper explores the application of Support Vector Machines (SVM) optimized by Particle Swarm Optimization (PSO) in predicting real estate prices. The study aims to improve the accuracy of real estate price forecasts by leveraging the combined strengths of SVM and PSO. The authors demonstrate the effectiveness of their proposed approach through experiments and analysis. The findings suggest that the SVM-PSO model can provide reliable predictions for real estate prices, offering potential benefits for real estate market analysis and decision-making.
d.	Price forecast in the competitive electricity market by support vector machine5)
The paper focuses on the use of Support Vector Machines (SVM) for predicting prices in the competitive electricity market. The study aims to enhance price forecasting accuracy in this dynamic market environment. The authors propose an SVM-based model and evaluate its performance using historical electricity price data. The findings highlight the effectiveness of the SVM approach in providing reliable price forecasts, thereby supporting decision-making in the competitive electricity market.
e.	Housing value forecasting based on machine learning methods6)
In this paper the researchers used housing data from various Boston suburbs with 13 features, including per capita crime rate. This paper argues that ANN’s are not an optimal method to use for predicting housing prices, because its accuracy is low and convergence speed is far from ideal. Instead, this paper uses different models including SVM, LSSVM and PLS to analyze housing values.         
The best performing model for this sample group was the Support Vector Machine, which had a mean square error estimate of 10.7373, while the other two took longer to run and had a larger mean square of error estimate. SVM performed better than LSSVM, due to LSSVM being a slightly more simplified mathematical version of SVM. The PLS algorithm's forecast results for the home values in the Boston suburb dataset were not satisfactory due to the significant presence of nonlinearity. Additionally, the computation efficiency of the PLS algorithm was relatively low. Several machine learning models should be constructed and analyzed, as this is a very important part of any analysis. The models should also be combined with corresponding characteristics of testing data to predict the housing values.
f.	A Comparison of Regression and Artificial Intelligence Methods in a Mass Appraisal Context7)
The paper presents a comparison analysis that assesses property prices in Louisville, Kentucky using several regression and machine learning methods. The dataset for this study consisted of 309,000 properties with 143 descriptive variables. In addition to this assessment data the researchers were also provided with a data set containing over 16,000 sales transactions. These large data sets gave them the ability to conduct a more comprehensive comparative study than had been conducted in the past. For the regression methods, the researchers used one traditional multiple regression and three non-traditional regression methods including a support vector machine (SVM). The models utilized in the AI (Artificial Intelligence) section of the study included neural networks (NN), radial basis function neural networks (RBFNN), and memory-based reasoning (MBR).                                   
The above models were all tested using five distinct error measures: MAE (Mean Absolute Error), RMSE (Root Mean Squared Error), RAE, RRSE, and R2. The study found that AI methods, particularly Artificial Neural Networks, outperformed traditional regression techniques in terms of prediction accuracy. The neural network models captured complex correlations between property attributes and market values, resulting in more accurate and reliable predictions.
4.	 Data Pre-Processing 
Our research focused on analyzing the impact of factors on Residential Sales Prices before and after the Covid pandemic. To achieve this, we utilized specific datasets, including Sales Data, Improvement Data, Improvement BuiltAs Data, and Appraisal Account Data. We filtered the Property type to include only Residential properties, and then narrowed down the data further by selecting residential property sales that occurred in 2019 and 2022.
For predicting Market Value, we utilized datasets such as Appraisal Account Type, Improvement, Improvement Built As, and Tax Account. We only considered the dataset for properties built on or after 2014 to 2023. To address any issues with outliers in the Market Value the final market value of properties was limited to sales between $100,000 and $30,000,000.
a.	Processing of NULL values
Before merging the property data tables, every data table was independently analyzed, and most variables with Null ratios of more than 30% were eliminated while the few relevant ones like Waterfront Type, View Quality are replaced with Null Values for ‘Not Applicable’. Also, the null values for variables such as ‘Attic Finished Square Feet,’ ‘Basement Square Feet,’ ‘Carport Square Feet’, ‘Porch Square Feet’, ‘Attached Garage Square Feet’, and ‘Detached Garage Square Feet’ were replaced with zero. In addition, the attributes with 10% to 30% of missing values such as Exterior, Interior, Roof Cover are replaced with the Null Value by assigning an additional category ‘Not Applicable’. Finally, we eliminated the rows with nulls in the columns less than 10% of Null Ratio columns to make sure that variables did not contain any null values,
b.	Converting the nominal, ordinal, date/time data to numbers 
1)	Nominal
Nominal variables are variables that have distinct categories or labels without any inherent order or ranking. The nominal variables HVAC Description, Utility Water, Utility Electric etc. were converted to dummy variables using one hot encoding. One-hot encoding creates separate binary features for each category, which allows the algorithm to treat each category independently.
2)	Ordinal
An ordinal attribute is a specific type of categorical data characterized by meaningful order or ranking among its categories. Unlike nominal attributes, which lack any inherent order and consist of distinct labels, ordinal attributes exhibit a clear relative ordering among their categories. In our dataset, the ‘view quality’, 'condition' and 'quality' attributes are ordinal variables and changed to numeric to capture the inherent order among categories.
Table1: Ordinal variables
Table	Ordinal attributes
Improvement	Condition: 0'Uninhabitable ~ 8 Excellent
Quality: 0 Low ~10 Excellent

3)	Dates
In the Pierce County dataset, we encountered date or year information represented as string types, necessitating their conversion to the appropriate date data type. This transformation was crucial to ensure accurate utilization of these columns in our analysis. 
In addition to removing variables with high Null Ratios, variables with no correlation to Sale Price and Market Value or variables with extremely high correlation to other variables were also removed. Once the initial set of important variables was identified the property data tables were joined with ‘Parcel Number’ into one master table. 
c.	Merging external datasets 
Incorporating school scores into our dataset can significantly enhance our analysis, as the quality of schools in the neighborhood directly influences house prices8). Given that our research is focused on Sales Price and Market Value for Residential properties, school scores serve as a crucial attribute to consider. Obtaining this valuable information required the integration of three datasets. Firstly, we sourced school scores from the School Digger 9) webpage, which provides a comprehensive list of elementary schools, their corresponding zip codes, and school scores ranging from 0 to 10. To merge the school dataset with master dataset, we utilized another dataset Address Points10) that contains ‘Tax Parcel Number’ and contains the relevant location (X, Y coordinate, Zip Code) information. The ‘Tax Parcel Number’ of the Address points was joined with the master property table ‘Parcel Number’. For each property, we then computed the average school score by considering all schools located within a 2-mile radius of the property and are in the same zip code.
Once all of the cleaning was done and all relevant variables were selected and the master property data set for 2019 Sales data consisted of 11,295 observations and 44 attributes while for 2022, Sales Data consisted of 10,247 with 44 attributes. For Market Value prediction, the final dataset consisted of 17,246 and 24 attributes. 
d.	Reduction of features using PCA (Primary Component Analysis) and 'Importances'.
PCA (Principal Component Analysis) is used for dimensionality reduction, which transforms the original features into uncorrelated principal components. In Principal Component Analysis (PCA), we have used the Scree Plots which is an effective way to understand and decide how many principal components we should retain for our analysis.  For instance, from the scree plot in Figure3, we can observe that the variance is decreasing with increase of components, however after k=28 variance value is becoming constant. So, k= 28 seems to be the appropriate component (feature) value, which we can use for further analysis. 
Figure3:Scree Plots(2019)		Figure4: PCA Scree Plots(2022)	    Figure5: Scree Plot(Market Value)
     
In our first problem statement, we aim to determine the most significant features that influence Residential property Sales Price both pre and post Covid. Feature selection will help us identify the most relevant features and their importance values, providing insights into any changes in feature importance and features between the Pre-Covid and Post-Covid periods. Another thing to note is that the more accurate our model is, the more we can trust feature important measures and other interpretations. For 2019 Sales data the important features (Figure 6) identified by Random Forest Regressor are 'Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score'. For 2022 Sales data the selected important features (Figure 7) are: 'Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Attached Garage Square Feet', 'Fireplaces', 'Built-As Square Feet', 'HVAC Description', 'Bedrooms', 'Physical Age', 'ZipCode','Average School Score'
For Market Value Prediction Data, the most important features (Figure 8) are:  'Land Net Square Feet', 'Land Gross Front Feet', 'View Quality', 'Waterfront Type', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score'
Figure 6: Feature Importance (2019) 			Figure 7: Feature Importance (2022)
  
Figure 8: Feature Importance (Market Value)
 

In conclusion, PCA and Feature Selection are two different techniques used for feature reduction and feature importance, respectively. Both techniques can be used to simplify the dataset and play a vital role in data preprocessing and model development.
e.	Use clustering for preprocessing
K-Means clustering is a popular unsupervised machine learning algorithm used for classification on unlabeled data. However, another application of clustering is in data preprocessing, where it is used to segment the data into more manageable and easier-to-predict subsets. In our case study, the data will be segmented into two distinct clusters that will be used for further analysis due to containing the most data.
Figure 9: Scatter Plot (2019)	Figure 10: Scatter Plot (2022)      	 Figure 11: Scatter Plot (Market Value)
   

The Silhouette Score is a metric we used to evaluate how well-separated the clusters are. It ranges from -1 to 1, with higher scores indicating better-defined and more distinct clusters. The Sales 2019 & 2022 dataset (Table 2) has a Silhouette score is 0.44 & 0.45, while the Market Value Dataset has a Silhouette score is 0.48, which suggests that the clusters in this dataset are relatively well-separated. From (Figure 9, 10, 11) we can see the distribution of Clusters for each dataset.
 



Table 2: K-means Clustering 
Data	Silhouette Score	Numbers of Records
		Cluster 1	Cluster 2
Sales Data 2019 	0.44	5077	6108
Sales Data 2022	0.45	5399	4729
Market Value Data	0.48	8278	9009

5.	Data Mining Models and Evaluations
a.	Evaluation Metrics: 
In this project, various machine learning algorithms are used in order predict the value of property; random forests, gradient boosting, and neural networks (Mora-Garcia, Cespedes-Lopez, & Perez-Sanchez, 2022). The metrics used for evaluation are R squared, Weighted Average
R-squared (R²) is a commonly used evaluation metric in machine learning for regression tasks. It measures the goodness of fit of a regression model, indicating how well the model explains the variance in the target variable (dependent variable) based on the input features (independent variables).
R-squared = 1 - (SSR (Sum of Squared Residuals) / SST)
where:
SSR (Sum of Squared Residuals) is the sum of the squared differences between the predicted values and the actual target values.
SST (Total Sum of Squares) is the sum of the squared differences between the actual target values and the mean of the target values.
To evaluate the performance of the models across various clustered data the weighted average of cluster results is calculated using the formula below where m = metric and r = # of rows for each cluster.
Weighted Average = (m1*r1 + m2*r2 + m3 * r3) / ∑ ri
k-fold Cross Validation: k-fold cross-validation is a model evaluation technique used to assess the performance of machine learning models. 
For both Sales and Market Value datasets, k-fold cross-validation with k=5 is applied. The data is divided into 5 folds, and the model is iteratively trained on 4 folds while testing on the remaining fold. This process is repeated 5 times, ensuring each fold is used for testing once. K-fold cross-validation helps prevent overfitting and provides a more reliable evaluation of the model's performance by avoiding dependency on a specific test and training dataset.
b.	Model Evaluations: 
Statement 1: 
Data mining and model evaluation is done to identify the best fitting machine learning models for sales data in 2019 and 2022. Initially, four machine learning models, namely Random Forest Regression, Gradient Boosting, Simple Vector Machines, and Neural Network Models, were deployed for each year's sales data. After hyperparameter tuning, the top-performing models were determined as the Random Forest Model (RFM) (n_estimators = 1000, min_samples_split=15) and Gradient Boosting (GBR) models(n_estimators = 750, min_samples_split=10) and Gradient Boosting (GBR) models, based on the evaluation metric R-squared, which calculates the percentage fit of the model to the data. 
For 2019 Sales data, the RFR and GBR models exhibited high R-squared values of 0.90 and 0.89. With three hidden layers (50, 40, 20) Neural Network has 0.80 R-squared which is less than RFR (Random Forest Regression) and GBR models.

Table 3: Results for Sales Dataset 2019 
Evaluation Metric: R Squared 
Model 	With All Features 	Feature Importance 	PCA 
Random Forest Regressor 	0.90 	0.90 	0.82  
Gradient Boosting Regressor 	0.89 	0.89 	0.80 
Neural Network  	0.80 	  	 0.72 
 
In addition, the Feature importance (Table 3) for RFM and GBR resulted in similar R-squared value which is 0.90 and 0.89 respectively. By identifying and selecting the most prominent features, the feature selection process provided valuable insights into which variables have the most significant impact on the Sales Price of Residential Properties pre-Covid. 
Subsequently, Principal Component Analysis (PCA) was applied to reduce the dimensionality of the data. This led to a decrease in the R-squared values for the RFM and GBR models in 2019 to 0.82 and 0.80, respectively. However, despite the reduction, the models still retained respectable predictive capabilities, indicating that PCA successfully preserved essential information while reducing the feature space.

Table 4: Clustering Results: Sales Dataset 2019
Evaluation Metric: R Squared
Model	Cluster 1	Cluster 2	Weighted Average
Random Forest Regressor 	0.87	0.91	0.89
Gradient Boosting Regressor	0.93	0.87	0.90
Neural Network	0.75	0.80	0.77

Table 5: Clustering Results (Feature Importance/PCA): Sales Dataset 2019  

Feature Importance: Evaluation Metric: R Squared   
Model   	Cluster 1   	Cluster 2   	Weighted Average   
Random Forest Regressor    	0.93  	0.78 	0.88  
Gradient Boosting Regressor   	0.87  	0.75 	0.82 
Principle Component Analysis: Evaluation Metric: R Squared 
Random Forest Regressor  	0.75 	0.82 	0.78 
Gradient Boosting Regressor 	0.74 	0.78 	0.76 
Neural Network  	0.60 	0.68 	0.77

Furthermore, for 2019 data, after applying K-Nearest Neighbor clustering into two groups, achieved a silhouette coefficient of 0.45, indicating moderately good clusters with some cohesion within the groups. However, some points showed proximity to other clusters or overlaps. These clusters performed well, with R-squared values of 0.89 for RFM and 0.90 for GBR (Table 5). However, after dimensional reduction techniques such as PCA were applied to these clusters, the R-squared values decreased to 0.78 for RFM and 0.76 for GBR. In addition, the models with important set of features still maintained 0.88 and 0.82 R squared values.
In the 2022 Sales data (Table 6), the RFM and GBR models first showed strong performance with R-squared values of 0.87 and 0.88, respectively. The three-layer (50, 40, 20) Neural Network model has an R-squared value of 0.82 which is slightly is slightly less than RFR and GBR.  In addition, the Feature selection for RFM and GBR resulted in similar R squared value which is 0.88 and 0.89 respectively. By identifying and selecting the most prominent features, the feature selection process provided valuable insights into which variables have the most significant impact on the Sales Price of Residential Properties post Covid.

Table 6: Results for Sales Dataset 2022
Evaluation Metric: R Squared
Model	With All Features	Feature Importance	PCA
Random Forest Regressor 	0.87	0.88	0.80
Gradient Boosting Regressor	0.88	0.89	0.83
Neural Network 	0.82	 	0.70

However, after applying PCA, the R-squared values decreased slightly to 0.80 for RFM and 0.83 for GBR. Nevertheless, both models retained considerable predictive power, suggesting that PCA effectively captured essential information while reducing the feature space. Overall, these results indicate that both models remain reliable for predicting sales trends in 2022, though with a slightly decreased level of accuracy compared to the original models without PCA. The Gradient Boosting model slightly outperformed the Random Forest model.
Table 7: Clustering Results: Sales Dataset 2022
Evaluation Metrics: R Squared
Model	Cluster 1	Cluster 2	Weighted Average
Random Forest Regressor 	0.75	0.82	0.77
Gradient Boosting Regressor	0.77	0.81	0.76
Neural Network 	0.70	0.81	0.75
 
Moreover, when creating two clusters for the 2022 data, the silhouette coefficient was 0.45, but these clusters exhibited the lowest R-squared values (Table 7), with a weighted average of 0.77 for RFM and 0.76 for GBR. After applying PCA to these clusters (Table 8), the R-squared values further decreased to 0.75 for RFM and 0.72 for GBR. In addition, the models with selected features have shown decreased R squared performance for both RFR and GBR.
Table 8: Clustering Results (Feature Importance/PCA): Sales Dataset 2022
Feature Importance: Evaluation Metric: R Squared 
Model 	Cluster 1 	Cluster 2 	Weighted Average 
Random Forest Regressor  	0.76 	0.70	0.72 
Gradient Boosting Regressor 	0.70 	0.65	0.67
PCA: Evaluation Metric: R Squared
Random Forest Regressor 	0.72	0.77	0.75
Gradient Boosting Regressor	0.75	0.73	0.72
Neural Network	0.61	0.62	0.60
In conclusion, the data mining and model evaluation process provided valuable insights into the performance of machine learning models for sales data in both 2019 and 2022 (Pre and Post Covid). The results demonstrate that both RFM and GBR models are reliable predictors for factors influencing Sales Price, and the application of PCA and clustering techniques influenced the models' accuracy, preserving essential information while reducing feature space. These findings contribute to a deeper understanding of the models' capabilities and aid in making data-driven decisions for sales forecasting and strategic planning.
Statement 2: 
For Market value prediction model, initially, four machine learning models, namely Random Forest Regression, Gradient Boosting, Simple Vector Machines, and Neural Network Models, were built. The Random Forest Model (RFM) (n_estimators = 750, min_samples_split=10) and Gradient Boosting (GBR) models (n_estimators = 500, min_samples_split=10) were determined as the top-performing models, based on the evaluation metric R-squared which shows how well the data fit the regression model. The RFR and GBR models exhited high R-squared values of 0.86 and 0.85 respectively. In addition, with hyperparameter tuning, the three-hidden layer (50, 50, 20) Neural Network model performed well with an R square value of 0.81 which also makes it one of the reliable predictors for market values.
Table 9: Results for Market Value Dataset 
Evaluation Metrics: R Squared
Model	With All Features	Feature Importance	PCA
Random Forest Regressor 	0.86	0.85	0.80
Gradient Boosting Regressor	0.85	0.83	0.75
Neural Network 	0.81	 	 0.78
 
Furthermore, with feature importance (Table 9), the RFR and GBR were built for selected features resulting in similar R squared value, which is 0.85 and 0.83, respectively. By identifying and selecting the most prominent features, the feature selection process will provide valuable insights into the most crucial features for predicting the market value for the houses built in the last ten years.
Subsequently, Principal Component Analysis (PCA) was applied to reduce the dimensionality of the data. This led to a decrease in the R-squared values for both the RFM and GBR models to 0.80 and 0.75, respectively. Interestingly, the Neural Network model surpassed the other toe models after PCA with an R-squared value of 0.78, indicating its enhanced performance in this context.
Table 10: Clustering Results: Market Value Dataset
Evaluation Metrics: R Squared
Model	Cluster 1	Cluster 2	Weighted Average
Random Forest 	0.90	0.82	0.87
Gradient Boosting 	0.87	0.79	0.84
Neural Network 	0.81	0.51	0.71

For clustering, we have assigned weights to the clusters and added their weighted evaluations to further decide if clusters helped us in improving the performance of the model or not. From the weighted average evaluations (Table 10), it can be observed that RFR and GBR performed well on the clustered dataset. However, for clustered dataset Neural Network didn’t perform well compared to RFR and GBR. However, feature importance (Table 11) resulted in lowered R squared for RFR i.e., 0.68, while GBR exhibited strong predictive capability with an R squared value of 0.86. Additionally, after PCA, the decrease in R squared values were observed as 0.82 for RFR, 0.79 for GBR. After PCA, Neural Network achieved the highest R squared value of 0.79.
Table 11: Clustering Results (Feature Importance/PCA): Market Value
Feature Importance: Evaluation Metric: R Squared  
Model  	Cluster 1  	Cluster 2  	Weighted Average  
Random Forest Regressor   	0.70	0.67	0.68
Gradient Boosting Regressor  	0.87	0.86	0.86
Principle Component Analysis: Evaluation Metric: R Squared
Random Forest Regressor 	0.87	0.72	0.82
Gradient Boosting Regressor	0.82	0.73	0.79
Neural Network 	0.84	0.68	0.79

In conclusion, we have observed that our Total Market Value prediction models in 2023, such as RFM and GBR, are performing well, demonstrating their reliability in accurately estimating the market value of properties. It can be assumed that the model is equally effective in predicting the Total Market Value for both clustered and non-clustered datasets with all the features. The reliable predictive capabilities of these models provide valuable insights for stakeholders, empowering data-driven decision-making in the real estate market.
6.	Discussion
In sales dataset, certain models stood out in performance, displaying their ability to fit the data. Notably, both the Random Forest Model (RFM) and Gradient Boosting (GBR) displayed the best fitting to the data. Additionally, the Neural Network model demonstrated commendable predictive capabilities. Through feature selection, the relevance of specific attributes in shaping sales prices was emphasized, encompassing periods both prior to and after the Covid-19 pandemic.
The use of Principal Component Analysis (PCA) for data dimensionality reduction resulted in marginal adjustments in model fitting for select models. While some information was inevitably lost, the models maintained significant predictive strength. Furthermore, the influence of PCA on the 2022 dataset became evident, particularly when considering the weighted average evaluation. This insight into post-dimensionality reduction performance contributed to a deeper understanding of the models' behavior.
Predicting total market value yielded valuable insights whereby several models consistently demonstrated good fitting to the data, indicating their capability to capture underlying trends (Jingyi Mu, Wu, & Zhang, 2014). Feature selection highlighted the contributions of specific attributes, notably those associated with properties constructed within the past decade.
The impact of PCA on dimensionality reduction was evident. Despite modest adjustments in model fitting for certain instances, the Neural Network model showed its adaptability after PCA, harnessing the reduced feature space. Additionally, the analysis of clustered data further unveiled patterns, affirming the models' capacity to predict total market value across various dataset characteristics.
Hyperparameter Tuning played a crucial role in refining model performance. It involved systematically adjusting parameters for the Random Forest Model (RFM), Gradient Boosting (GBR), and Neural Network models. This process optimized their ability to fit the data and enhance predictive accuracy. Successful hyperparameter tuning ensured consistent model fitting and improved reliability in capturing property value dynamics across the different periods.
In sum, this research underscores the dependability of specific models in predicting property’s marketvalues, bridging the gap between pre- and post-Covid-19 periods (Grybauskas, Pilinkienė, & Stundžienė, 2021). Their consistent fitting to the data provides valuable insights, guiding informed decision-making within the evolving real estate landscape. Understanding how preprocessing techniques, such as PCA and clustering, impact model performance empowers businesses to make well-informed choices, enhancing their overall efficacy and adaptability within the real estate industry. Lastly, the models' ability to consistently capture property value changes across diverse times reinforces their significance within the evolving post-pandemic real estate context. This significance extends not only to model dependability but also to securing storage and computation capabilities in understanding economic shocks like Covid-19 to an industry with such a large data availability (Roy, Roy, & Mahmood, 2021).
7.	Conclusion 
This study aims at using applied machine learning techniques to identify the significant factors influencing residential property sales prices before and after Covid, as well as predicting the market value of residential houses in 2023. Discoveries and new insights were made at each phase, observing that both Random Forest and Gradient Boosting Regressor outperformed other models. The feature importance helps us understand significant factors for both post and pre Covid, and how different they are.
Conclusively, several factors that influence residential property Sale Prices remained consistent between 2019 and 2022, hence their enduring importance in the real estate market. The factors include land size, quality, porch and attached garage square footage, built-as square footage, physical age, zip code, and average school score. However, the post-Covid period brought about changes in buyer preferences leading to the inclusion of new attributes. For instance, the significance of energy-efficient HVAC systems like heat pumps highlighted preference for environmentally friendly energy options. The number of bedrooms gained importance, reflecting the need for home offices, or changing family requirements.
The emergence of fireplaces as an influential attribute suggested increased emphasis on home comfort, highlighting evolving buyer priorities post-Covid. Additionally, it was observed that proximity to schools with higher average school scores impacted residential property prices, reflecting families' preferences for quality education. Understanding these similarities and differences is essential for real estate professionals and stakeholders in navigating the evolving market and making informed decisions during an economic shock like Covid-19.
Our predictive market value machine learning model has yielded an R-squared value of 0.87. This indicates that approximately 87% of the variability in market values is effectively captured by our model's selected variables, highlighting its strong predictive capabilities and potential for making accurate market value forecasts. In addition, from feature importance revealed the most important feature as the Quality of the property.  School ratings are still a prominent feature. The study discovered that regardless of the dataset, the random forest and Gradient Boosting models consistently demonstrated superior accuracy. This finding contradicted previous research which had suggested that Neural Networks outperformed these models [Jian Guan, Alan S. Levitan, Jozef Zurada. (2011)].
An intriguing observation emerged when using clustering as a preprocessing technique during data re-structuring. Notably, the R-squared values for 2019 Sales Data and Market Value prediction datasets remain the same, reflecting the effectiveness of this approach. However, for the 2022 Sales Data, clustering did not yield favorable results. The study also suggests a potential avenue for further exploration: investigating the efficacy of density-based clustering through the DBSCAN algorithm.
Additionally, there is a significant amount of data on the Pierce County open data portal, although we were constrained by time, with more time one could include many additional community variables. Also, our research did not consider demographic information about the homeowner, that information could also be used as features in future studies. The findings in this paper are both interesting and impactful. For researchers, it provides a roadmap for constructing effective prediction algorithms that can be used in future research.
Constraints such as computation power was a major issue since the dataset size required more memory and time, especially when machine learning models were being deployed and developed. It is important to consider a wider scope of variables and a wider temporal resolution of dataset to be able to find more insights and patterns. The more data used to train the models, the better the predictive ability.


 
References
1.	Property Information (2023, June 23). | Pierce County, WA - Official Website. (n.d.). https://www.piercecountywa.gov/736/Data-Downloads  
2.	Grybauskas, A., Pilinkien Vaida& Stundien Alina. (2021, August 3). Predictive analytics using big data for the real estate market during the COVID-19 pandemic - journal of big data. SpringerOpen. https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00476-0 
3.	Mora-Garcia, R.-T., Cespedes-Lopez, M.-F., & Perez-Sanchez, V. R. (2022, November 21). Housing price prediction using machine learning algorithms in COVID-19 times. MDPI. https://www.mdpi.com/2073-445X/11/11/2100  
4.	Wang, X., Wen, J., Zhang, Y., & Wang, Y. (2014). Real estate price forecasting based on SVM optimized by PSO. Optik, 125(3), 1439-1443.  
5.	Gao, C., Bompard, E., Napoli, R., & Cheng, H. (2007). Price forecast in the competitive electricity market by support vector machine. Physica A: Statistical Mechanics and its Applications, 382(1), 98-113.  
6.	Jingyi Mu, Fang Wu, Aihua Zhang. (2014). Housing value forecasting based on machine learning methods. In Abstract and Applied Analysis (Vol. 2014). Hindawi. https://www.hindawi.com/journals/aaa/2014/648047/ 
7.	Jian Guan, Alan S. Levitan, Jozef Zurada. (2011). A Comparison of Regression and Artificial Intelligence Methods in a Mass Appraisal Context. Journal of Real Estate Research, 33(3), 349-387 https://www.jstor.org/stable/24888380 
8.	Ward, J. (2022, August 24). How school ratings can affect home values and prices. Home Sweet Homes. https://www.homes.com/blog/2022/08/how-school-ratings-can-affect-home-values-and-prices/ 
9.	 School score (2023, July 05). SchoolDigger. (n.d.). https://www.schooldigger.com/go/WA/schools/0870001496/school.aspx 
10.	 Address points(2023, July 05). Pierce County WA Open GeoSpatial Data Portal (v2.1). (n.d.-a). https://gisdata-piercecowa.opendata.arcgis.com/datasets/address-points/expl
Appendices
Code:
Statement 1 Code: 

# Import relevant libraries
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import collections
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, make_scorer
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

####################################################################
# Utility Functions
####################################################################

def DecisionTreeRegressorModel(X_test, y_test, X_train, y_train):
    t_start = time.time()
    dtr =  DecisionTreeRegressor(max_leaf_nodes = 500, max_features='auto', splitter='best', random_state = 0)
    dtr_mean_score = np.mean(cross_val_score(dtr, X_train, y_train, cv=5))
    print(f'Mean Score for Decision Tree: {dtr_mean_score:.4f}')
    dtr.fit(X_train, y_train)
    y_pred_dtr = dtr.predict(X_test)
    mae = metrics.mean_absolute_error(y_test, y_pred_dtr)
    mse = metrics.mean_squared_error(y_test,y_pred_dtr)
    rmse = np.sqrt(mse)
    print('**Performance Evaluations for Decision Tree**')
    print(f'mae: {mae:.4f}')
    print(f'R2 score from the Decision Tree model: {r2_score(y_test, y_pred_dtr):.2f}')
    print (f'mse: {mse:.4f}')
    print(f'rmse: {rmse:.4f}')
    t_end = time.time()
    print(f'Execution time for DTR: {t_end-t_start:.2f} seconds')
    return dtr
    
def RandomForestRegressorModel(X_test, y_test, X_train, y_train):
    t_start = time.time()
    rfrm = RandomForestRegressor(random_state=1234) 
    rfrm_mean_score = np.mean(cross_val_score(rfrm, X_train, y_train, cv=5))
    print(f'Mean Score for RandomForest: {rfrm_mean_score:.4f}')
    rfrm.fit(X_train, y_train)
    y_pred_rfrm = rfrm.predict(X_test) 
    rfrm_mean_score = np.mean(cross_val_score(rfrm, X_train, y_train, cv=5))
    print(f'Mean Score for Random Forest: {rfrm_mean_score:.4f}')

    # Calculate MSE (mean squared errors), RMSE (Root Mean Squred Error),and R^2 for errors.
    mse = metrics.mean_squared_error(y_test,y_pred_rfrm)
    rmse = np.sqrt(mse)
    print('\n', '**Performance Evaluation for Random Forest Regressor**')
    print (' mse: ', mse,'\n','rmse:', rmse)
    print(f'R2 score from the Random Forest model: {r2_score(y_test, y_pred_rfrm):.2f}')
    t_end = time.time()
    print(f'Execution time for RFR: {t_end-t_start:.2f} seconds')
    return rfrm
    
def GradientBoostingRegressorModel(X_test, y_test, X_train, y_train):
    t_start = time.time()
    gbr = GradientBoostingRegressor(random_state = 1234)
    
    gbr_mean_score = np.mean(cross_val_score(gbr, X_train, y_train, cv=5))
    print(f'Mean Score for GradientBoosting: {gbr_mean_score:.4f}'
    gbr.fit(X_train, y_train)
    y_pred_gbr = gbr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_gbr)
    print(print('**Performance Evaluations for Gradient Boosting**'))
    print(' mse: ', mse,'\n')
    print(f'R2 score from the Gradient Boosting model: {r2_score(y_test, y_pred_gbr):.2f}')
    t_end = time.time()
    print(f'Execution time for GBR: {t_end-t_start:.2f} seconds')
    return gbr
    
def SVMRegressorModel(kernel_Value, C_Value, epsilon_value, X_test, y_test, X_train, y_train):
    t_start = time.time()
    svr = SVR(kernel=kernel_Value, C= C_Value, epsilon=epsilon_value)
    svr_mean_score = np.mean(cross_val_score(svr, X_train, y_train, cv=5))
    print(f'Mean Score for SVM: {svr_mean_score:.4f}')
    
    # Train the model
    svr.fit(X_train, y_train)
    # Generate some test data
    #X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
    y_pred_svr = svr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_svr)
    print('**Performance Evaluations for SVM Regressor**')
    print(' mse: ', mse,'\n')
    print(f'R2 score from the SVM model: {r2_score(y_test, y_pred_svr):.2f}')
    t_end = time.time()
    print(f'Execution time for SVM: {t_end-t_start:.2f} seconds')
    return svr

def NeuralNetworkRegressorModel(X_test, y_test, X_train, y_train):    
    nnr = MLPRegressor(random_state=1234)
    nnr_mean_score = np.mean(cross_val_score(nnr, X_train, y_train, cv=5))
    print(f'Mean Score for Neural Network: {nnr_mean_score:.4f}')
    nnr.fit(X_train,y_train)
    y_pred_nn = nnr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print(f'R2 score from the NN model: {r2_score(y_test, y_pred_nn):.2f}')
    return nnr

def NeuralNetworkModel_1Layer(hidden_layer_sizes, activationFunc, X_test, y_test, X_train, y_train):
    t_start = time.time()
    nnr = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes),
                       max_iter=3000, activation=activationFunc, random_state=1234)
    nnr.fit(X_train,y_train)
    y_pred_nn = nnr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print(f'R2 score from the NN model One Layer: {activationFunc} {hidden_layer_sizes} {r2_score(y_test, y_pred_nn):.2f}')
    t_end = time.time()
    print(f'Execution time for NNR_1L: {t_end-t_start:.2f} seconds')

def NeuralNetworkModel_2Layer(hidden_layer_sizes, hidden_layer_2, activationFunc, X_test, y_test, X_train, y_train):
    t_start = time.time()
    model = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes, hidden_layer_2), max_iter=1000,  activation=activationFunc, random_state=0)
    model.fit(X_train, y_train)
    y_pred_nn = model.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print(f'R2 score from the NN model Three Layers: {activationFunc} {hidden_layer_sizes} {hidden_layer_2} {r2_score(y_test, y_pred_nn):.2f}')
    t_end = time.time()
    print(f'Execution time for NNR_2L: {t_end-t_start:.2f} seconds')
    

def NeuralNetworkModel_3Layer(hidden_layer_sizes, hidden_layer_2, hidden_layer_3, activationFunc,  X_test, y_test, X_train, y_train):
    t_start = time.time()
    model = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes, hidden_layer_2, hidden_layer_3), max_iter=1000, 
                         activation=activationFunc, random_state=0)
    model.fit(X_train, y_train)
    y_pred_nn = model.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print(f'R2 score from the NN model Two Layers: {activationFunc} {hidden_layer_sizes} {hidden_layer_2} {hidden_layer_3} {r2_score(y_test, y_pred_nn):.2f}')
    t_end = time.time()
    print(f'Execution time for NNR_2L: {t_end-t_start:.2f} seconds')
    
 
def FindNeuralNetworkParam_GridSearch(X_test, y_test, X_train, y_train):
    t_start = time.time()
    r2_scorer = make_scorer(r2_score)
    param_grid = {
        'hidden_layer_sizes': [(40), (50), (100), (150), (50, 40), (100, 50), (150, 100),
                               (50, 40, 10), (50, 40, 20), (50, 40, 30), (50, 40, 40)],
        'activation': ['relu'],  # Activation function for hidden layers
        'max_iter': [4000,5000]
    }

    nnm_r = MLPRegressor()
    grid_src = GridSearchCV(estimator= nnm_r, param_grid = param_grid, cv=5, scoring=r2_scorer)
    grid_src.fit(X_train,y_train)
    t_end = time.time()
    
    print('\n\n **Report**')
    print("Best Parameters: ", grid_src.best_params_)
    print("Best Mean Test R-squared Score: ", grid_src.best_score_)
    print(f'The best estimator: {grid_src.best_estimator_}')
    print(f'The best parameters:\n {grid_src.best_params_}')
    print(f'The best score: {grid_src.best_score_:.4f}')
    print(f'Total run time for GridSearchCV: {(t_end-t_start):.2f} seconds')
    # Check the details of search
    return pd.DataFrame(grid_src.cv_results_)

def RandomForestRegressorModel_WithParams(nEstimator, minSamplesSplit, X_test, y_test, X_train, y_train):
    t_start = time.time()
    rfrm = RandomForestRegressor(n_estimators=nEstimator, min_samples_split=minSamplesSplit, random_state=1234)
    rfrm_mean_score = np.mean(cross_val_score(rfrm, X_train, y_train, cv=5))
    print(f'Mean Score for RandomForest: {rfrm_mean_score:.4f}')
    
    rfrm.fit(X_train, y_train)
    y_pred_rfrm = rfrm.predict(X_test)
    # Calculate MSE (mean squared errors), RMSE (Root Mean Squred Error),and R^2 for errors.
    mse = metrics.mean_squared_error(y_test,y_pred_rfrm)
    mape = np.mean(np.abs((y_test - y_pred_rfrm)/y_test))*100
    rmse = np.sqrt(mse)

    print('\n', '**Performance Evaluation for Random Forest Regressor**')
    print (' mse: ', mse,'\n','rmse:', rmse)
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the Random Forest model: {r2_score(y_test, y_pred_rfrm):.2f}')
    t_end = time.time()
    print(f'Execution time for RFR: {t_end-t_start:.2f} seconds')
    return rfrm

def FindRandomForestRegressorParam_GridSearch(X_test, y_test, X_train, y_train):
    t_start = time.time()
    r2_scorer = make_scorer(r2_score)
    param_grid = {
                'n_estimator':[500, 750, 1000, 1500],
                'min_samples_split': [5, 10, 15, 20], 
             }
    rfrm = RandomForestRegressor(random_state = 1234)
    grid_src = GridSearchCV(estimator= rfrm, param_grid = param_grid, cv=5, scoring=r2_scorer)
    grid_src.fit(X_train,y_train)
    t_end = time.time()
    
    print('\n\n **Report**')
    print("Best Parameters: ", grid_src.best_params_)
    print("Best Mean Test R-squared Score: ", grid_src.best_score_)
    print(f'The best estimator: {grid_src.best_estimator_}')
    print(f'The best parameters:\n {grid_src.best_params_}')
    print(f'The best score: {grid_src.best_score_:.4f}')
    print(f'Total run time for GridSearchCV: {(t_end-t_start):.2f} seconds')
    # Check the details of search
    return pd.DataFrame(grid_src.cv_results_)

def GradientBoostingRegressorModel_WithParams(nEstimator, minSamplesSplit, X_test, y_test, X_train, y_train):
    t_start = time.time()
    gbr = GradientBoostingRegressor(n_estimators=nEstimator, min_samples_split=minSamplesSplit)
    gbr_mean_score = np.mean(cross_val_score(gbr, X_train, y_train, cv=5))
    print(f'Mean Score for GradientBoostingWithParams: {gbr_mean_score:.4f}')
    
    gbr.fit(X_train, y_train)
    y_pred_gbr = gbr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_gbr)
    mape = np.mean(np.abs((y_test - y_pred_gbr)/y_test))*100
    print(print('**Performance Evaluations for Gradient Boosting**'))
    print(' mse: ', mse,'\n')
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the Gradient Boosting model WithParams: {r2_score(y_test, y_pred_gbr):.2f}')
    t_end = time.time()
    print(f'Execution time for GBR: {t_end-t_start:.2f} seconds')
    return gbr

def FindGradientBoostingRegressorParam_GridSearch(X_test, y_test, X_train, y_train):
    t_start = time.time()
    r2_scorer = make_scorer(r2_score)
    param_grid = {
                'n_estimator':[500, 750, 1000, 1500],
                'min_samples_split': [5, 10, 15, 20], 
             }
    gbr = GradientBoostingRegressor(random_state = 1234)
    grid_src = GridSearchCV(estimator= gbr, param_grid = param_grid, cv=5, scoring=r2_scorer)
    grid_src.fit(X_train,y_train)
    t_end = time.time()
    
    print('\n\n **Report**')
    print("Best Parameters: ", grid_src.best_params_)
    print("Best Mean Test R-squared Score: ", grid_src.best_score_)
    print(f'The best estimator: {grid_src.best_estimator_}')
    print(f'The best parameters:\n {grid_src.best_params_}')
    print(f'The best score: {grid_src.best_score_:.4f}')
    print(f'Total run time for GridSearchCV: {(t_end-t_start):.2f} seconds')
    # Check the details of search
    return pd.DataFrame(grid_src.cv_results_)

def AnalyzingDatasets(df):        
    ## show the first five rows of the Appraisal dataset
    print("First 5 rows of the dataset:")
    print(df.head())

    # Check the data types of each column
    print("Data types:")
    print(df.dtypes)

    # Check the number of missing values in each column
    print("Missing values:")
    print(df.isnull().sum())
    df.info()

    # Get the summary statistics of numeric variables (appraisal dataset)
    print("Summary statistics:")
    print(df.describe())

    # Distinct Count & frequency of nominal variables
    df.nunique()

    for colName in df.columns:
        print("UniqueValues for " + colName)
        print(df[colName].value_counts())
    
    #null values present in the Appraisal dataset
    df.isnull().sum().sort_values(ascending=False)


def DetermineCorrelatedVariables(df):
    df_numeric = df.select_dtypes(include=np.number)
    corrMatrix = df_numeric.corr()
    threshold = 0.7
    highly_correlated_variables = []

    # Iterate through each column in the correlation matrix
    for i in range(len(df.columns)):
        for j in range(i + 1, len(corrMatrix.columns)):
            if abs(corrMatrix.iloc[i, j]) >= threshold:
                # Append the names of highly correlated variables to the list
                variable_i = corrMatrix.columns[i]
                variable_j = corrMatrix.columns[j]
                highly_correlated_variables.append((variable_i, variable_j))

    return highly_correlated_variables

    
def CalculateAverageSchoolScore(dfMergedRow, schoolData):
    schoolRadius = 2*5280   # schools in 2 mile (5280 feet) radius
    dfMergedX = dfMergedRow['X']
    dfMergedY = dfMergedRow['Y']
    zipCode = dfMergedRow['ZipCode']
    
    filteredSchoolData = schoolData[(schoolData['X_COORD'] <= dfMergedX + schoolRadius) 
                               & (schoolData['X_COORD'] >= dfMergedX - schoolRadius) 
                               & (schoolData['Y_COORD'] <= dfMergedY + schoolRadius) 
                               & (schoolData['Y_COORD'] >= dfMergedY - schoolRadius)
                               & (schoolData['ZIP'] == zipCode)]
    if len(filteredSchoolData) == 0:
        return 0
    else:
        return filteredSchoolData["score"].mean()
    

####################################################################
# 1. Reading the Datasets
####################################################################

input_file_appraisal = 'appraisal_account.txt'
column_names_appraisal = ['Parcel Number', 'Appraisal Account Type', 'Business Name', 'Value Area ID',
                 'Land Economic Area', 'Buildings', 'Group Account Number', 
                 'Land Gross Acres', 'Land Net Acres', 'Land Gross Square Feet', 
                 'Land Net Square Feet', 'Land Gross Front Feet', 'Land Width', 'Land Depth', 
                 'Submerged Area Square Feet', 'Appraisal Date', 'Waterfront Type', 
                 'View Quality', 'Utility Electric', 'Utility Sewer', 'Utility Water', 
                 'Street Type', 'Latitude', 'Longitude']

input_file_improvement = 'improvement.txt'
column_names_improvement = ['Parcel Number', 'Building ID', 'Property Type', 'Neighborhood',
                 'Neighborhood Extension', 'Square Feet', 'Net Square Feet', 
                 'Percent Complete', 'Condition', 'Quality', 'Primary Occupancy Code', 
                 'Primary Occupancy Description', 'Mobile Home Serial Number', 'Mobile Home Total Length', 
                 'Mobile Home Make', 'Attic Finished Square Feet', 'Basement Square Feet', 
                 'Basement Finished Square Feet', 'Carport Square Feet', 'Balcony Square Feet', 
                 'Porch Square Feet', 'Attached Garage Square Feet', 'Detached Garage Square Feet', 
                 'Fireplaces', 'Basement Garage Door']

input_file_improvementBuiltAs = 'improvement_builtas.txt'
column_names_improvementBuiltAs = ['Parcel Number', 'Building ID', 'Built-As Number', 'Built-As ID',
                 'Built-As Description', 'Built-As Square Feet', 'HVAC', 'HVAC Description', 
                 'Exterior', 'Interior', 'Stories', 'Story Height', 'Sprinkler Square Feet', 
                 'Roof Cover', 'Bedrooms', 'Bathrooms', 'Units', 'Class Code', 'Class Description', 
                 'Year Built', 'Year Remodeled', 'Adjusted Year Built', 'Physical Age', 
                 'Built-As Length', 'Built-As Width', 'Mobile Home Mode']


input_file_sale = 'sale.txt'
column_names_sale = ['ETN', 'Parcel Count', 'Parcel Number', 'Sale Date',
                 'Sale Price', 'Deed Type', 'Grantor', 'Grantee', 
                 'Valid/Invalid', 'Confirmed/Uncomfirmed', 'Exclude Reason', 
                 'Improved/Vacant', 'Appraisal Account Type']
                                


# Set the delimiter used in the input file
delimiter = '|'  # Example: Tab-separated values ('\t'), Comma-separated values (','), etc.

# Read the delimited text file into a pandas DataFrame
df_appraisal = pd.read_csv(input_file_appraisal, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_appraisal)
df_improvement = pd.read_csv(input_file_improvement, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_improvement)
df_improvementBuiltAs = pd.read_csv(input_file_improvementBuiltAs, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_improvementBuiltAs)
df_sale = pd.read_csv(input_file_sale, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_sale)

# Read external Dataset files
df_Address_Points = pd.read_csv('Address_Points.csv')
df_school_Data= pd.read_csv("school_by_zipcode.csv")

####################################################################
# 2. Datasets Preprocessing
####################################################################

# Analyzing Datasets
print("Analyzing Appraisal Data Set")
AnalyzingDatasets(df_appraisal)

print("Analyzing Improvement Data Set")
AnalyzingDatasets(df_improvement)

print("Analyzing Improvement BuiltAs Data Set")
AnalyzingDatasets(df_improvementBuiltAs)

print("Analyzing Sales Data Set")
AnalyzingDatasets(df_sale)

print("Analyzing AddressPoints Data Set")
AnalyzingDatasets(df_Address_Points)

print("Analyzing School Data Set")
AnalyzingDatasets(df_school_Data)

####################################################################
# 2.1 Appraisal Dataset Preprocessing
####################################################################

# Filtering the dataset to only include Residential Properties
appraisal_dfFiltered = df_appraisal[(df_appraisal['Appraisal Account Type'] == 'Residential')]
appraisal_dfFiltered.info()
appraisal_dfFiltered.isnull().sum().sort_values(ascending=False) 
appraisal_dfFiltered.dtypes


# Dropping the non-required columns which have more than 30% nulls, and not informative
appraisal_dfFiltered.drop(['Appraisal Account Type', 'Value Area ID', 'Business Name', 'Submerged Area Square Feet', 
                           'Group Account Number', 'Land Economic Area', 'Appraisal Date'] , axis=1, inplace=True)

appraisal_dfFiltered['Waterfront Type'] = appraisal_dfFiltered['Waterfront Type'].apply(lambda x: 1 if not pd.isnull(x) else 0)

appraisal_dfFiltered['View Quality'] = appraisal_dfFiltered['View Quality'].fillna('N/A')

viewQualityOrd = {'View Quality': {'N/A':0, 'View Lim -':1,'View Lim':2,'View Lim +':3, 
                    'View Good' :4,'View Good +':5, 'View Avg':6,'View Avg +':7,
                      'View V-Good':8,'View V-Good +':9}}
appraisal_dfFiltered = appraisal_dfFiltered.replace(viewQualityOrd)

# Converting Parcel number to String
appraisal_dfFiltered['Parcel Number']=(appraisal_dfFiltered['Parcel Number']).apply(str)

# Finding the correlated variables
numericCorrelatedVariables = DetermineCorrelatedVariables(appraisal_dfFiltered)
print(numericCorrelatedVariables)

# Dropping additional variables based on correlation analysis
appraisal_dfFiltered.drop(['Land Depth', 'Land Gross Acres', 
                           'Land Gross Square Feet', 'Land Width', 'Land Net Acres'] , axis=1, inplace=True)

#Covert nominal variables into dummy
df_appraisalFinal = pd.get_dummies(appraisal_dfFiltered, columns=['Utility Electric', 
                 'Utility Sewer', 'Utility Water', 'Street Type'], drop_first=True)

df_appraisalFinal.info()

# Finding if there are duplicate parcel numbers in appraisal data set
print([item for item, count in collections.Counter(df_appraisalFinal['Parcel Number']).items() if count > 1])

####################################################################
# 2.2 Improvement Dataset Preprocessing
####################################################################

# Filtering datset to consider only residential properties
df_improvement = df_improvement[(df_improvement['Property Type'] == 'Residential')]

# Dropping the non-required columns which have majority of nulls, not informative
df_improvement.drop(['Mobile Home Serial Number', 'Mobile Home Total Length', 
                   'Mobile Home Make', 'Basement Garage Door', 'Neighborhood', 'Neighborhood Extension', 'Primary Occupancy Code', 
                   'Primary Occupancy Description', 'Property Type' ] , axis=1, inplace=True)

# Populating null values with 0
df_improvement['Attic Finished Square Feet'] = df_improvement['Attic Finished Square Feet'].fillna(0)
df_improvement['Basement Square Feet'] = df_improvement['Basement Square Feet'].fillna(0)
df_improvement['Carport Square Feet'] = df_improvement['Carport Square Feet'].fillna(0)
df_improvement['Balcony Square Feet'] = df_improvement['Balcony Square Feet'].fillna(0)
df_improvement['Porch Square Feet'] = df_improvement['Porch Square Feet'].fillna(0)
df_improvement['Attached Garage Square Feet'] = df_improvement['Attached Garage Square Feet'].fillna(0)
df_improvement['Detached Garage Square Feet'] = df_improvement['Detached Garage Square Feet'].fillna(0)
df_improvement['Fireplaces'] = df_improvement['Fireplaces'].fillna(0)


df_improvement.isnull().sum().sort_values(ascending=False)

# Removing rows with null values
df_improvement =  df_improvement[~df_improvement['Condition'].isnull()]

# Check the data types of each column
print("Data types:")
print(df_improvement.dtypes)

# Converting Parcel number from Integer to String
df_improvement['Parcel Number']=(df_improvement['Parcel Number']).apply(str)

# Finding Correlated Variables
numericCorrelatedVariables = DetermineCorrelatedVariables(df_improvement)
print(numericCorrelatedVariables)

# Dropping the correlated columns for analysis
df_improvement.drop(['Square Feet','Basement Finished Square Feet', 
                   'Net Square Feet'] , axis=1, inplace=True)

# Converting Ordinal Variables Condition and Quality to Numeric Variables
conditionOrd = {'Condition': {'Uninhabitable':0, 'Extra Poor':1,'Very Poor':2,'Poor':3, 
                      'low':4,'Fair':5, 'Average':6, 'Avg' : 6, 'Avg.':6,'Good':7, 'Excellent':8}}

qualityOrd = {'Quality': {'Low':0, 'Low Plus':1,'Fair':2,'Fair Plus':3, 
                    'Average' :4,'Average Plus':5, 'Good':6,'Good Plus':7,
                      'Very Good':8,'Very Good Plus':9,'Excellent':10}}

df_improvement = df_improvement.replace(conditionOrd)
df_improvement = df_improvement.replace(qualityOrd)

# Finding if there are duplicate parcel numbers in improvement data set
print([item for item, count in collections.Counter(df_improvement['Parcel Number']).items() if count > 1])

####################################################################
# 2.3 Sales Dataset Preprocessing
####################################################################
# Filter the Dataset where Appraisal Account Type is Residential
df_sale_Filtered = df_sale[(df_sale['Appraisal Account Type'] == 'Residential')]

# Converting Sale Date from String to Date Time Object and Parcel Number as string
df_sale_Filtered['Sale Date'] = pd.to_datetime(df_sale_Filtered['Sale Date'])
df_sale_Filtered['Parcel Number']=(df_sale_Filtered['Parcel Number']).apply(str)

# Filtering dataset to consider only sales for 2019 and 2022
df_sale_Filtered = df_sale_Filtered[(df_sale_Filtered['Sale Date'].dt.year == 2019) | 
                                     (df_sale_Filtered['Sale Date'].dt.year == 2022)]                           
 
df_sale_Filtered.isnull().sum().sort_values(ascending=False)

# Check the data types of each column
print("Data types:")
print(df_sale_Filtered.dtypes)

# Dropping non required columns
df_sale_Filtered.drop(['ETN', 'Exclude Reason', 'Grantor', 'Grantee', 'Deed Type', 'Valid/Invalid', 
                       'Appraisal Account Type', 'Confirmed/Uncomfirmed', 'Improved/Vacant'] , axis=1, inplace=True)

# Finding if there are duplicate parcel numbers in improvement data set
print([item for item, count in collections.Counter(df_sale_Filtered['Parcel Number']).items() if count > 1])

# For the duplicate sales records, choosing the latest sales record by sorting the dataset based on sale date and then
# selecting the last record 
df_sale_Filtered = df_sale_Filtered.sort_values(by=['Sale Date'], ascending=True)
df_sale_Filtered = df_sale_Filtered.drop_duplicates(subset=['Parcel Number'], keep='last')

# Finding corrrelated variables
numericCorrelatedVariables = DetermineCorrelatedVariables(df_sale)
print(numericCorrelatedVariables)

####################################################################
# 2.4 ImprovementBuiltAs Dataset Preprocessing
####################################################################
df_improvementBuiltAs.info()
df_improvementBuiltAs.isnull().sum().sort_values(ascending=False)

# Converting Parcel number from integer to string
df_improvementBuiltAs['Parcel Number']=(df_improvementBuiltAs['Parcel Number']).apply(str)

# Populating Null values for Exterior with Not Applicable
df_improvementBuiltAs['Exterior'] = df_improvementBuiltAs['Exterior'].fillna('Not Applicable')

# Populating Null values for Interior with Not Applicable
df_improvementBuiltAs['Interior'] = df_improvementBuiltAs['Interior'].fillna('Not Applicable')

# Populating Null values for Roof Cover with Not Applicable
df_improvementBuiltAs['Roof Cover'] = df_improvementBuiltAs['Roof Cover'].fillna('Not Applicable')

df_improvementBuiltAs.info()
df_improvementBuiltAs.isnull().sum().sort_values(ascending=False) 
df_improvementBuiltAs.dtypes

# Dropping coulmns withmajority of nulls, non-informative 
df_improvementBuiltAs.drop(['Built-As Number', 'Built-As ID','HVAC', 
                    'Mobile Home Mode','Class Description', 'Class Code', 'Year Built',
                  'Adjusted Year Built'] , axis=1, inplace=True)

df_improvementBuiltAs =  df_improvementBuiltAs[~df_improvementBuiltAs['Physical Age'].isnull()]
df_improvementBuiltAs = df_improvementBuiltAs[~df_improvementBuiltAs['Story Height'].isnull()]  

# Dropping columns not required for analysis
df_improvementBuiltAs.drop(['Exterior', 'Interior','Roof Cover', ] , axis=1, inplace=True)

# Finding corrrelated variables
numericCorrelatedVariables = DetermineCorrelatedVariables(df_improvementBuiltAs)
print(numericCorrelatedVariables)

# Removing highly correlated variables
df_improvementBuiltAs.drop(['Sprinkler Square Feet', 'Built-As Length', 'Built-As Width'] , axis=1, inplace=True)

#Covert nominal variables into dummy
df_improvementBuiltAsFinal =  pd.get_dummies(df_improvementBuiltAs, columns=['HVAC Description'], 
                                  drop_first=True)

# Finding if there are duplicate parcel numbers in improvement data set
print([item for item, count in collections.Counter(df_improvementBuiltAsFinal['Parcel Number']).items() if count > 1])

####################################################################
# 2.4 Address Points Dataset Preprocessing
####################################################################
df_Address_Points.info()

# Dropping Non required columns
df_Address_Points.drop(['OBJECTID', 'Address', 'Mail_Stop', 'City', 'State', 'Last_Edited', 
                       'Status', 'HouseNumber', 'PrefixDirectional', 'StreetName', 'StreetType',
                       'PostDirectional', 'Jurisdiction', 'AddressID'] , axis=1, inplace=True)

df_Address_Points.isnull().sum().sort_values(ascending=False) 
df_Address_Points =  df_Address_Points[~df_Address_Points['TaxParcelNumber'].isnull()]


print([item for item, count in collections.Counter(df_Address_Points['TaxParcelNumber']).items() if count > 1])
# Removing rows having duplicate Parcel number
df_Address_Points = df_Address_Points.drop_duplicates(subset=['TaxParcelNumber'], keep='last')

####################################################################
# 2.5 School Dataset Preprocessing
####################################################################
df_school_Data.info()

# Dropping Non required columns
df_school_Data.drop(['X', 'Y', 'OBJECTID', 'NAME', 'ADDRESS', 'CITY', 'DISTRICT',
                       'DIST_NO', 'TYPE', 'PHONE', 'WEBSITE', 'PRS_ID', 'GRADE'] , axis=1, inplace=True)

df_school_Data.isnull().sum().sort_values(ascending=False) 


####################################################################
# 3 Merging various datasets
####################################################################

df_appraisalFinal.info()
df_improvement.info()
df_sale_Filtered.info()
df_improvementBuiltAs.info()

# Merging sales and appraisal datasets
# Since there were no duplicate rows having same Parcel Number, merging the datasets using innner join to avoid any rows with null values
df_merged = pd.merge(df_appraisalFinal, df_sale_Filtered, left_on='Parcel Number', right_on='Parcel Number', how='inner')
df_merged.info()

# Merging improvement and improvement BuiltAs dataset
# Merging datasets using left join to consider all rows of improvement table
df_improvement_merged = pd.merge(df_improvement, df_improvementBuiltAsFinal, left_on=['Parcel Number', 'Building ID'], 
                                 right_on=['Parcel Number', 'Building ID'], how='left')

# Dropping Building ID after merging as it is not required for further analysis
df_improvement_merged.drop(['Building ID'] , axis=1, inplace=True)


# Finding if there are duplicate parcel numbers in improvement merged data sets, for duplicate values
# select the property row which got remodelled in the last
df_improvement_merged= df_improvement_merged.sort_values(by=['Year Remodeled'], ascending=True)
print([item for item, count in collections.Counter(df_improvement_merged['Parcel Number']).items() if count > 1])
df_improvement_merged = df_improvement_merged.drop_duplicates(subset=['Parcel Number'], keep= 'last')


# Merging the improvement merged data set with the merged dataset
df_merged = pd.merge(df_merged, df_improvement_merged, left_on='Parcel Number', 
                     right_on='Parcel Number', how='inner')

df_merged.info()


# Finding if there are duplicate parcel numbers in merged dataset
print([item for item, count in collections.Counter(df_merged['Parcel Number']).items() if count > 1])
df_merged.isnull().sum().sort_values(ascending=False) 

# Finding the correlation matric for the merged dataset for highly correlated variables
numericCorrelatedVariables = DetermineCorrelatedVariables(df_merged)
print(numericCorrelatedVariables)

# Dropping columns from merged datasets which are not relevant for analysis
df_merged.drop(['Built-As Description', 'Buildings', 'Units', 'Parcel Count' ] , axis=1, inplace=True)


# Merging the merged dataset with address points to get the property coordinates information
# Merged the datasets using inner join to consider only properties which are present in both
df_merged = pd.merge(df_merged, df_Address_Points, left_on='Parcel Number', right_on='TaxParcelNumber', how='inner')

# Merging with merged dataset with school dataset

# For each property calculate the average school score of the all the schools which are within 2 mile property radius
# and are present in same zipcode
df_merged['Average School Score']  = df_merged.apply(lambda row: CalculateAverageSchoolScore(row, 
                                        df_school_Data), axis = 1)

df_merged.isnull().sum().sort_values(ascending=False) 

# Dropping Non-Required Columns from the merged datasets
df_merged.drop(['Utility Electric_POWER INSTALLED', 'Utility Electric_POWER NO - COMMENT',  'Utility Sewer_SEWER/SEPTIC INSTALLED', 'Utility Sewer_SEWER/SEPTIC NO',  'Utility Water_WATER INSTALLED', 'Utility Water_WATER NO', 'Street Type_STREET NO ROAD',  'Street Type_STREET UNPAVED', 'Utility Sewer_SEWER/SEPTIC NO PERC', 'Parcel Number',  'Longitude', 'Latitude', 'X', 'Y', 'TaxParcelNumber',  'Year Remodeled', 'Bathrooms', 'Story Height',   'Percent Complete'], axis=1, inplace=True)

# Analyze the correlated variables in the merged datasets
numericCorrelatedVariables = DetermineCorrelatedVariables(df_merged)
print(numericCorrelatedVariables)  

df_merged.info()

####################################################################
# 4 Analyzing Pre-Covid (2019) and Post-Covid (2022) Datasets
####################################################################

# Splitting the datasets into pre and post covid datasets based on sales date
df_merged2019 = df_merged[(df_merged['Sale Date'].dt.year == 2019)] 
df_merged2022 = df_merged[(df_merged['Sale Date'].dt.year == 2022)]

# Dropping Non required columns
df_merged2019.drop(['Sale Date'] , axis=1, inplace=True)
df_merged2022.drop(['Sale Date'] , axis=1, inplace=True)

df_merged2019.columns
df_merged2022.columns

####################################################################
# 4.1 Analyzing Pre-Covid (2019) Datasets
####################################################################

# X and Y variables for 2019 Dataset
X_2019 = df_merged2019.loc[:, df_merged2019.columns != 'Sale Price']
y_2019 = df_merged2019[['Sale Price']].values.ravel()
fn_2019 = X_2019.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_2019)
X_s_2019 = scaler.transform(X_2019)

# Divide the scaled dataset into training and testing data
X_train_s_2019, X_test_s_2019, y_train_2019, y_test_2019 = train_test_split(X_s_2019, y_2019, test_size =.30, random_state=1234) 

# Divide the non-scaled dataset into training and testing data.
X_train_2019, X_test_2019, y_train_2019, y_test_2019 = train_test_split(X_2019, y_2019, test_size =.30, random_state=1234) 

####################################################################
# 4.1.1 Building Models With Default & Hyperparameters Parameters

# Decision Tree Regressor
dtr_2019 = DecisionTreeRegressorModel(X_test_2019, y_test_2019, X_train_2019, y_train_2019)
# Random Forest Regressor
rfr_2019 = RandomForestRegressorModel(X_test_2019, y_test_2019, X_train_2019, y_train_2019)
# Grid Seach for best Random Forest Regressor
FindRandomForestRegressorParam_GridSearch(X_test, y_test, X_train, y_train)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_test, y_test, X_train, y_train)
# Gradient Boosting Regression
grbr_2019 = GradientBoostingRegressorModel(X_test_2019, y_test_2019, X_train_2019, y_train_2019)
# Grid Seach for best Gradient Boosting
FindGradientBoostingRegressorParam_GridSearch(X_test, y_test, X_train, y_train)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_test, y_test, X_train, y_train)
# SVM Regression
svmr_2019 = SVMRegressorModel('linear', 10, 0.01, X_test_s_2019, y_test_2019, X_train_s_2019, y_train_2019)
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_s_2019, y_test_2019, X_train_s_2019, y_train_2019)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2019 = FindNeuralNetworkParam_GridSearch(X_test_s_2019, y_test_2019, X_train_s_2019, y_train_2019)
results_gs = pd.DataFrame(gridSearch_2019.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', X_test_s_2019,  y_test_2019, X_train_s_2019, y_train_2019)

Output:
Mean Score for Decision Tree: 0.8338
**Performance Evaluations for Decision Tree**
R2 score from the Decision Tree model: 0.87
Mean Score for RandomForest: 0.8670
** Performance Evaluations for  Random Forest Regressor**
R2 score from the Random Forest model: 0.85
Mean Score for RandomForestWithParams: 0.8990
R2 score from the Random Forest modelWithParams: 0.89
Mean Score for GradientBoosting: 0.7913
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.80
Mean Score for GradientBoostingmodelwithParams: 0.8521
R2 score from the GradientBoostingmodelWithParamsl: 0.82
Mean Score for SVM: -0.0321
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: -0.03
Mean Score for Neural Network: -0.2060
**Performance Evaluations for Neural Network**
R2 score from the NN model: -0.21
The best estimators: MLPClassifier(hidden_layer_sizes=(50, 40, 20))
The best parameters for Layers:
 {'activation': 'relu', 'hidden_layer_sizes': (50, 40, 20)}
The best score for Layers: 0.8043

####################################################################
# 4.1.2 Feature Selection 
####################################################################
####################################################################
# 4.1.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_2019.feature_importances_
np.sum(importances)
plt.barh(fn_2019, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_2019, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_reduced = selector.fit_transform(X_2019, y_2019)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_2019):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test \
       = train_test_split(X_reduced, y_2019, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)


Output: 
** 8 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for RandomForest: 0.8634
**Performance Metrics for Random Forest Regressor**
R2 score from the Random Forest model: 0.84
 
Mean Score for RandomForestWithParams: 0.914
R2 score from the Random Forest modelWithParams: 0.90

####################################################################
# 4.1.2.1 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_2019.feature_importances_
np.sum(importances)
plt.barh(fn_2019, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_2019, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_reduced = selector.fit_transform(X_2019, y_2019)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_2019):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test \
       = train_test_split(X_reduced, y_2019, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_test, y_test, X_train, y_train)

Output:
Selected features for GBR are:
 ['Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for GradientBoosting: 0.8275
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.81
Execution time for GBR: 3.72 seconds
 
Mean Score for GradientBoostingWithParams: 0.8531
R2 score from the Gradient Boosting modelWithParams 0.89
####################################################################
# 4.1.3 PCA
####################################################################

# Create an instance PCA and build the model using Xn.
# We start from the same number of components as the number of original features.
pca_prep = PCA().fit(X_s_2019)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 28  
n_pc = 28
pca_2019 = PCA(n_components = n_pc).fit(X_s_2019)
Xp_2019 = pca_2019.transform(X_s_2019)
print(f'After PCA, we use {pca_2019.n_components_} components.\n')

# Split the data into training and testing subsets.
Xp_train_2019, Xp_test_2019, yp_train_2019, yp_test_2019 = train_test_split(Xp_2019, y_2019, test_size =.3, random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_2019 = RandomForestRegressorModel(Xp_test_2019, yp_test_2019, Xp_train_2019, yp_train_2019)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_2019, yp_test_2019, Xp_train_2019, yp_train_2019)

# Gradient Boosting models using the transformed data. 
gbr_2019 = GradientBoostingRegressorModel(Xp_test_2019, yp_test_2019, Xp_train_2019, yp_train_2019)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_2019, yp_test_2019, Xp_train_2019, yp_train_2019)

# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_2019, yp_test_2019, Xp_train_2019, yp_train_2019)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_2019, yp_test_2019,  Xp_train_2019, yp_train_2019)

Output:

  

Mean Score for RandomForest: 0.7903
 **Performance Evaluation for Random Forest Regressor**
R2 score from the Random Forest model: 0.80
Execution time for RFR: 170.88 seconds
Mean Score for RandomForestWithParams: 0.8249
 R2 score from the Random Forest modelWithParams: 0.82
Mean Score for GradientBoosting: 0.7638
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.79
Mean Score for GradientBoostingWithParams: 0.8009
R2 score from the Gradient Boosting modelWithParams 0.81
Mean Score for SVM: -0.0321
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: -0.03
Mean Score for Neural Network: -0.2114
**Performance Evaluations for Neural Network**
R2 score from the NN model: -0.22
**Performance Evaluations for Neural Network**
R2 score from the NN model Three Layers: relu 50 40 20 0.72


####################################################################
# 4.1.4 Clustering
####################################################################

n_pc = 2
pca_2019 = PCA(n_components = n_pc).fit(X_s_2019)
Xp_2019_2Comp = pca_2019.transform(X_s_2019)

# Create an instance (object) of the KMeans class with the parameters
# initialized (cluster count 2)
km_2019 = KMeans(n_clusters=2, random_state=1234)
# Build a model.
km_2019 = km_2019.fit_predict(Xp_2019_2Comp)

silhouette_avg = silhouette_score(Xp_2019_2Comp, km_2019)
print('Silhouette Score:', silhouette_avg)

# Splitting dataset into two clusters
c0_2019 = df_merged2019[km_2019 == 0]
c1_2019 = df_merged2019[km_2019 == 1]
c0_2019.shape 
c1_2019.shape

Output:
Silhouette Score: 0.45084582614538116
 (6823, 44)
 (4472, 44)
  
           
####################################################################
# 4.1.4.1 Analyzing Cluster 0
####################################################################

plt.scatter(Xp_2019_2Comp[:, 0], Xp_2019_2Comp[:, 1], c=km_2019, s=50, cmap='viridis')
plt.show()

X_C02019 = c0_2019.loc[:, c0_2019.columns != 'Sale Price']
y_C02019 = c0_2019[['Sale Price']].values.ravel()
fn_C02019 = X_C02019.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C02019)
X_C0s_2019 = scaler.transform(X_C02019)

####################################################################
# 4.1.4.1.1  Building Models Wth Default Parameters for Cluster 0
####################################################################

# Divide the scaled dataset into training and testing data
X_train_C0s_2019, X_test_C0s_2019, y_train_C02019, y_test_C02019 = train_test_split(X_C0s_2019, y_C02019, test_size =.30 ,random_state=1234) 

# Divide the unscaled dataset into training and testing data.
X_train_C02019, X_test_C02019, y_train_C02019, y_test_C02019 = train_test_split(X_C02019, y_C02019, test_size =.30, random_state=1234) 

# Decision Tree Regressor
dtr_C02019 = DecisionTreeRegressorModel(X_test_C02019, y_test_C02019, X_train_C02019, y_train_C02019)
# Random Forest Regressor
rfr_C02019 = RandomForestRegressorModel(X_test_C02019, y_test_C02019, X_train_C02019, y_train_C02019)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15 X_test_C02019, y_test_C02019, X_train_C02019, y_train_C02019)

# Gradient Boosting Regression
grbr_C02019 = GradientBoostingRegressorModel(X_test_C02019, y_test_C02019, X_train_C02019, y_train_C02019)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_test_C02019, y_test_C02019, X_train_C02019, y_train_C02019))

# SVM Regression
svmr_C02019 = SVMRegressorModel('linear', 10, 0.01, X_test_C0s_2019, y_test_C02019, X_train_C0s_2019, y_train_C02019)
# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C0s_2019, y_test_C02019, X_train_C0s_2019, y_train_C02019)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2019 = FindNeuralNetworkParam_GridSearch(X_test_C0s_2019, y_test_C02019, X_train_C0s_2019, y_train_C02019)
results_gs = pd.DataFrame(gridSearch_2019.cv_results_)

# Evaluating Neural network results with grid search best estimator values
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', X_test_C0s_2019, y_test_C02019, X_train_C0s_2019, y_train_C02019)

Output:

Mean Score for Decision Tree: 0.7991
**Performance Evaluations for Decision Tree**
R2 score from the Decision Tree model: 0.9287
Mean Score for RandomForest: 0.8113
**Evaluation of Errors for Random Forest Regressor**
R2 score from the Random Forest model: 0.82
Mean Score for RandomForestWithParams: 0.8613
R2 score from the Random Forest modelWithParams: 0.87
Mean Score for GradientBoosting: 0.8705
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.89
Mean Score for GradientBoostingWithParams: 0.9225
R2 score from the Gradient Boosting modelWithParams 0.93
Mean Score for SVM: -0.0753
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: -0.08
Mean Score for Neural Network: -0.2622
**Performance Evaluations for Neural Network**
R2 score from the NN model: -0.27
R2 score from the NN model Three Layers: relu 50 40 20 0.75

####################################################################
# 4.1.4.1.2 Feature Selection for cluster 0
####################################################################

####################################################################
# 4.1.4.1.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C02019.feature_importances_
np.sum(importances)
plt.barh(fn_C02019, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C02019, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C0_reduced = selector.fit_transform(X_C02019, y_C02019)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C02019):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C02019, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

Output:

** 9 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Stories', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for RandomForest: 0.8710
R2 score from the Random Forest model: 0.89 

 
Mean Score for RandomForestWithParams: 0.9219
 R2 score from the Random Forest modelWithParams: 0.95
####################################################################
# 4.1.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C02019.feature_importances_
np.sum(importances)
plt.barh(fn_C02019, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C02019, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C0_reduced = selector.fit_transform(X_C02019, y_C02019)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C02019):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C02019, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

Output: 
** 6 features are selected.
Selected features for GBR are:
 ['Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for GradientBoosting: 0.7963
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.82

 
Mean Score for GradientBoostingWithParams: 0.8516
R2 score from the Gradient Boosting modelWithParams 0.88
####################################################################
# 4.1.4.1.3 PCA For Cluster 0
####################################################################

pca_prep = PCA().fit(X_C0s_2019)
pca_prep.n_components_

pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 26  
n_pc = 26
pca_C02019 = PCA(n_components = n_pc).fit(X_C0s_2019)
Xp_C02019 = pca_C02019.transform(X_C0s_2019)
print(f'After PCA, we use {pca_C02019.n_components_} components.\n')

# Split the data into training and testing subsets.
Xp_train_C02019, Xp_test_C02019, yp_train_C02019, yp_test_C02019 = train_test_split(Xp_C02019, y_C02019, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C02019 = RandomForestRegressorModel(Xp_test_C02019, yp_test_C02019, Xp_train_C02019, yp_train_C02019)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_C02019, yp_test_C02019, Xp_train_C02019, yp_train_C02019)


# For Gradient Boosting 
gbr_C02019 = GradientBoostingRegressorModel(Xp_test_C02019, yp_test_C02019, Xp_train_C02019, yp_train_C02019)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_C02019, yp_test_C02019, Xp_train_C02019, yp_train_C02019)

# SVM Regression
svmr_C02019 = SVMRegressorModel('linear', 10, 0.01, Xp_test_C02019, yp_test_C02019, Xp_train_C02019, yp_train_C02019)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C02019, yp_test_C02019, Xp_train_C02019, yp_train_C02019)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_C02019, yp_test_C02019, Xp_train_C02019, yp_train_C02019)

Output: 
             

Mean Score for Random Forest: 0.7171
 **Performance Evaluation for Random Forest Regressor**
R2 score from the Random Forest model: 0.75
Mean Score for RandomForestWithParams: 0.7854
 R2 score from the Random Forest modelWithParams: 0.80
Mean Score for Gradient Boosting: 0.6918
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.70
Mean Score for GradientBoostingWithParams: 0.7215
R2 score from the Gradient Boosting modelWithParams 0.74
Mean Score for SVM: -0.0730
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: -0.09
Mean Score for Neural Network: 0.2220
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.26
R2 score from the NN model Three Layers: relu 50 40 20 0.6032

####################################################################
# 4.1.4.2 Analyzing Cluster 1
####################################################################

X_C12019 = c1_2019.loc[:, c1_2019.columns != 'Sale Price']
y_C12019 = c1_2019[['Sale Price']].values.ravel()
fn_C12019 = X_C12019.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C12019)
X_C1s_2019 = scaler.transform(X_C12019)


# Divide the dataset into training and testing data.
# Divide the scaled dataset into training and testing data
X_train_C1s_2019, X_test_C1s_2019, y_train_C12019, y_test_C12019 = train_test_split(X_C1s_2019, y_C12019, test_size =.30,random_state=1234) 

# Divide the unscaled dataset into training and testing data.
X_train_C12019, X_test_C12019, y_train_C12019, y_test_C12019 = train_test_split(X_C12019, y_C12019, test_size =.30,random_state=1234) 

####################################################################
# 4.1.4.2.1  Building Models Wth Default Parameters for Cluster 1
####################################################################

# Decision Tree Regressor
dtr_C12019 = DecisionTreeRegressorModel(X_test_C12019, y_test_C12019, X_train_C12019, y_train_C12019)
# Random Forest Regressor
rfr_C12019 = RandomForestRegressorModel(X_test_C12019, y_test_C12019, X_train_C12019, y_train_C12019)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_test_C12019, y_test_C12019, X_train_C12019, y_train_C12019)

# Gradient Boosting Regression
grbr_C12019 = GradientBoostingRegressorModel(X_test_C12019, y_test_C12019, X_train_C12019, y_train_C12019)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_test_C12019, y_test_C12019, X_train_C12019, y_train_C12019)

# SVM Regression
svmr_C12019 = SVMRegressorModel('linear', 10, 0.01, X_test_C1s_2019, y_test_C12019, X_train_C1s_2019, y_train_C12019)

# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C1s_2019, y_test_C12019, X_train_C1s_2019, y_train_C12019)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2019 = FindNeuralNetworkParam_GridSearch(X_test_C1s_2019, y_test_C12019, X_train_C1s_2019, y_train_C12019)
results_gs = pd.DataFrame(gridSearch_2019.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', X_test_C1s_2019, y_test_C12019, X_train_C1s_2019, y_train_C12019)

Output:

Mean Score for Decision Tree: 0.8846
**Performance Evaluations for Decision Tree**
R2 score from the Decision Tree model: 0.83
Mean Score for RandomForest: 0.8487
**Evaluation of Errors for Random Forest Regressor**
R2 score from the Random Forest model: 0.87
Mean Score for RandomForestWithParams: 0.9216
 R2 score from the Random Forest modelWithParams: 0.91
Mean Score for GradientBoosting: 0.8265
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.84
Mean Score for GradientBoostingWithParams: 0. 8896
R2 score from the Gradient Boosting modelWithParams 0.87
Mean Score for SVM: 0.0283
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.02
Mean Score for Neural Network: 0.23
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.21
R2 score from the NN model Three Layers: relu 50 40 20 0.61


####################################################################
# 4.1.4.2.2 Feature Selection for cluster 1
####################################################################

####################################################################
# 4.1.4.2.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C12019.feature_importances_
np.sum(importances)
plt.barh(fn_C12019, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C12019, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C1_reduced = selector.fit_transform(X_C12019, y_C12019)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C12019):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C12019, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

Output: 

** 8 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Basement Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for RandomForest: 0.6964
 **Performance Evaluation for Random Forest Regressor**
R2 score from the Random Forest model: 0.71
Mean Score for RandomForestWithParams: 0.7423
 R2 score from the Random Forest modelWithParams: 0.75

####################################################################
# 4.1.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C12019.feature_importances_
np.sum(importances)
plt.barh(fn_C12019, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C12019, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C1_reduced = selector.fit_transform(X_C12019, y_C12019)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C12019):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C12019, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train) 

Output: 
** 6 features are selected.
Selected features for GBR are:
 ['Land Net Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for GradientBoosting: 0.7109
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.73
Mean Score for GradientBoostingWithParams: 0.7697
R2 score from the Gradient Boosting modelWithParams 0.78


####################################################################
# 4.1.4.2.3 PCA For Cluster 1
####################################################################

pca_prep = PCA().fit(X_C1s_2019)
pca_prep.n_components_

pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.title('Scree Plot 2019 Cluster2')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 26  
n_pc = 26
pca_C12019 = PCA(n_components = n_pc).fit(X_C1s_2019)
Xp_C12019 = pca_C12019.transform(X_C1s_2019)
print(f'After PCA, we use {pca_C12019.n_components_} components.\n')

# Split the data into training and testing subsets.
Xp_train_C12019, Xp_test_C12019, yp_train_C12019, yp_test_C12019 = train_test_split(Xp_C12019, y_C12019, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C12019 = RandomForestRegressorModel(Xp_test_C12019, yp_test_C12019, Xp_train_C12019, yp_train_C12019)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_C12019, yp_test_C12019, Xp_train_C12019, yp_train_C12019)

# For Gradient Boosting 
gbr_C12019 = GradientBoostingRegressorModel(Xp_test_C12019, yp_test_C12019, Xp_train_C12019, yp_train_C12019)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_C12019, yp_test_C12019, Xp_train_C12019, yp_train_C12019)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C12019, yp_test_C12019, Xp_train_C12019, yp_train_C12019)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_C12019, yp_test_C12019, Xp_train_C12019, yp_train_C12019)  

Output: 

 

Mean Score for Random Forest: 0.7734
 **Performance Metrics for Random Forest Regressor**
R2 score from the Random Forest model: 0.79
Mean Score for RandomForestWithParams: 0.8596
 R2 score from the Random Forest modelWithParams: 0.82
Mean Score for GradientBoosting: 0.7245
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.74
Mean Score for GradientBoostingWithParams: 0.7720
R2 score from the Gradient Boosting modelWithParams 0.79
Mean Score for Neural Network: 0.30
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.28
R2 score from the NN model Three Layers: relu 50 40 20 0.69

####################################################################
# 4.2 Analyzing Post-Covid (2022) Datasets
####################################################################

X_2022 = df_merged2022.loc[:, df_merged2022.columns != 'Sale Price']
y_2022 = df_merged2022[['Sale Price']].values.ravel()
fn_2022 = X_2022.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_2022)
X_s_2022 = scaler.transform(X_2022)

# Divide the scaled dataset into training and testing data
X_train_s_2022, X_test_s_2022, y_train_2022, y_test_2022 = train_test_split(X_s_2022, y_2022, test_size =.30,random_state=1234) 

# Divide the unscaled dataset into training and testing data.
X_train_2022, X_test_2022, y_train_2022, y_test_2022 = train_test_split(X_2022, y_2022, test_size =.30,random_state=1234) 

####################################################################
# 4.2.1 Building Models Wth Default Parameters
####################################################################

# Decision Tree Regressor
dtr_2022 = DecisionTreeRegressorModel(X_test_2022, y_test_2022, X_train_2022, y_train_2022)
# Random Forest Regressor
rfr_2022 = RandomForestRegressorModel(X_test_2022, y_test_2022, X_train_2022, y_train_2022)

# Grid Seach for best Random Forest Regressor
FindRandomForestRegressorParam_GridSearch(X_test_2022, y_test_2022, X_train_2022, y_train_2022)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_test_2022, y_test_2022, X_train_2022, y_train_2022)
# Gradient Boosting Regression
gbr_2022 = GradientBoostingRegressorModel(X_test_2022, y_test_2022, X_train_2022, y_train_2022)

# Grid Seach for best Gradient Boosting
FindGradientBoostingRegressorParam_GridSearch(X_test_2022, y_test_2022, X_train_2022, y_train_2022)

#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(500, 10, X_test_2022, y_test_2022, X_train_2022, y_train_2022)
# SVM Regression
svmr_2022 = SVMRegressorModel('linear', 10, 0.01, X_test_s_2022, y_test_2022, X_train_s_2022, y_train_2022)

# NN Regression
nnr_2022 = NeuralNetworkRegressorModel(X_test_s_2022, y_test_2022, X_train_s_2022, y_train_2022)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2022 = FindNeuralNetworkParam_GridSearch( X_test_s_2022, y_test_2022, X_train_s_2022, y_train_2022)
results_gs = pd.DataFrame(gridSearch_2022.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare_2022 = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', X_test_s_2022, y_test_2022, X_train_s_2022, y_train_2022)

Output: 
Mean Score for Decision Tree: 0.8879
**Performance Evaluations for Decision Tree**
R2 score from the Decision Tree model: 0.85
Mean Score for RandomForest: 0.8010
** Performance Evaluations for  Random Forest Regressor**
R2 score from the Random Forest model: 0.82
Mean Score for RandomForestWithParams: 0.8562
 R2 score from the Random Forest modelWithParams: 0.87
Mean Score for GradientBoosting: 0.8109
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.84
Mean Score for GradientBoostingWithParams: 0.8720
R2 score from the Gradient Boosting modelWithParams 0.88
Mean Score for SVM: 0.142
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.02
Mean Score for Neural Network: -0.2060
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.18
The best estimators: MLPClassifier(hidden_layer_sizes=(50, 40, 20))
The best parameters for Layers:
 {'activation': 'relu', 'hidden_layer_sizes': (50, 40, 20)}
The best score for Layers: 0.83

####################################################################
# 4.2.2 Feature Selection 
####################################################################

####################################################################
# 4.2.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County Dataset with their importance values.
importances = rfr_2022.feature_importances_
np.sum(importances)
plt.barh(fn_2022, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_2022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(1)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)

plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold=0.015)
X_reduced = selector.fit_transform(X_2022, y_2022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_2022):
    if i: selected_features.append(j)
print(f'Selected features are:\n {selected_features}')

# Build a model using reduced number of features
X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test \
       = train_test_split(X_reduced, y_2022, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

Output:
  

** 11 features are selected.
Selected features are:
 ['Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Attached Garage Square Feet', 'Fireplaces', 'Built-As Square Feet', 'Bedrooms', 'Physical Age', 'HVAC Description_Heat Pump', 'ZipCode', 'Average School Score']
Mean Score for RandomForest: 0.7976
R2 score from the Random Forest modelWithParams: 0.81
Mean Score for RandomForestWithParams: 0.8438
R2 score from the Random Forest modelWithParams: 0.86

####################################################################
# 4.2.2.2 Feature Selection Using Gradient Booster Regressor
####################################################################

# Find the significant features for Pierce County Data with their importance values.
importances = gbr_2022.feature_importances_
np.sum(importances)
plt.barh(fn_2022, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_2022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_reduced = selector.fit_transform(X_2022, y_2022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_2022):
    if i: selected_features.append(j)
print(f'Selected features for 2022 GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test \
       = train_test_split(X_reduced, y_2022, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

Output:

  

** 8 features are selected.
Selected features for 2022 GBR are:
 ['Land Net Square Feet', 'Quality', 'Fireplaces', 'Built-As Square Feet', 'Physical Age', 'HVAC Description_Heat Pump', 'ZipCode', 'Average School Score']
Mean Score for GradientBoosting: 0.8143
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.83
Mean Score for GradientBoostingWithParams: 0.8598
R2 score from the GradientBoostingWithParams: 0.88

####################################################################
# 4.2.3 PCA
####################################################################

pca_prep = PCA().fit(X_s_2022)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 28  
n_pc = 28
pca_2022 = PCA(n_components = n_pc).fit(X_s_2022)
Xp_2022 = pca_2022.transform(X_s_2022)
print(f'After PCA, we use {pca_2022.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train_2022, Xp_test_2022, yp_train_2022, yp_test_2022 = train_test_split(Xp_2022, y_2022, test_size =.3, random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_2022 = RandomForestRegressorModel(Xp_test_2022, yp_test_2022, Xp_train_2022, yp_train_2022)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_2022, yp_test_2022, Xp_train_2022, yp_train_2022)

# For Gradient Boosting 
gbr_2022 = GradientBoostingRegressorModel(Xp_test_2022, yp_test_2022, Xp_train_2022, yp_train_2022)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_2022, yp_test_2022, Xp_train_2022, yp_train_2022)

# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_2022, yp_test_2022, Xp_train_2022, yp_train_2022)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_2022, yp_test_2022,  Xp_train_2022, yp_train_2022)

Output:

  
Mean Score for RandomForest: 0.7388
**Performance Metrics Random Forest Regressor**
R2 score from the Random Forest model: 0.76
Mean Score for RandomForestWithParams: 0.7987
 R2 score from the Random Forest modelWithParams: 0.80
Mean Score for GradientBoosting: 0.7965
R2 score from the Gradient Boosting model: 0.81
Mean Score for GradientBoostingWithParams: 0.8213
R2 score from the Gradient Boosting modelWithParams 0.84
Mean Score for SVM: 0.0142
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.02
Mean Score for Neural Network: 0.22
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.19
R2 score from the NN model Three Layers: relu 50 40 20 0.71

####################################################################
# 4.2.4 Clustering
####################################################################

n_pc = 2
pca_2022 = PCA(n_components = n_pc).fit(X_s_2022)
Xp_2022_2Comp = pca_2022.transform(X_s_2022)

# Create an instance (object) of the KMeans class with the parameters
# initialized (cluster count 2)
km_2022 = KMeans(n_clusters=2, random_state=1234)
# Build a model.
km_2022 = km_2022.fit_predict(Xp_2022_2Comp)

silhouette_avg = silhouette_score(Xp_2022_2Comp, km_2022)
print('Silhouette Score:', silhouette_avg)

# Splitting dataset into two clusters
c0_2022 = df_merged2022[km_2022 == 0]
c1_2022 = df_merged2022[km_2022 == 1]
c0_2022.shape
c1_2022.shape
plt.scatter(Xp_2022_2Comp[:, 0], Xp_2022_2Comp[:, 1], c=km_2022, s=50, cmap='viridis')
plt.show()

Output:
Silhouette Score: 0.45787567925493555
(4341, 44)
(5906, 44)
  
                
####################################################################
# 4.2.4.1 Analyzing Cluster 0
####################################################################

X_C02022 = c0_2022.loc[:, c0_2022.columns != 'Sale Price']
y_C02022 = c0_2022[['Sale Price']].values.ravel()
fn_C02022 = X_C02022.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C02022)
X_C0s_2022 = scaler.transform(X_C02022)

####################################################################
# 4.2.4.1.1  Building Models Wth Default Parameters for Cluster 0
####################################################################

# Divide the scaled dataset into training and testing data
X_train_C0s_2022, X_test_C0s_2022, y_train_C02022, y_test_C02022 = train_test_split(X_C0s_2022, y_C02022, test_size =.30,random_state=1234) 

# Divide the dataset into training and testing data.
X_train_C02022, X_test_C02022, y_train_C02022, y_test_C02022 = train_test_split(X_C02022, y_C02022, test_size =.30,random_state=1234) 

# Decision Tree Regressor
dtr_C02022 = DecisionTreeRegressorModel(X_test_C02022, y_test_C02022, X_train_C02022, y_train_C02022)
# Random Forest Regressor
rfr_C02022 = RandomForestRegressorModel(X_test_C02022, y_test_C02022, X_train_C02022, y_train_C02022)
# Grid Seach for best Random Forest Regressor
FindRandomForestRegressorParam_GridSearch(X_test_C02022, y_test_C02022, X_train_C02022, y_train_C02022)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_test_C02022, y_test_C02022, X_train_C02022, y_train_C02022)
# Gradient Boosting Regression
grbr_C02022 = GradientBoostingRegressorModel(X_test_C02022, y_test_C02022, X_train_C02022, y_train_C02022)
# Grid Seach for best Gradient Boosting
FindGradientBoostingRegressorParam_GridSearch(X_test_C02022, y_test_C02022, X_train_C02022, y_train_C02022)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_test_C02022, y_test_C02022, X_train_C02022, y_train_C02022)
# SVM Regression
svmr_C02022 = SVMRegressorModel('linear', 10, 0.01, X_test_C0s_2022, y_test_C02022, X_train_C0s_2022, y_train_C02022)

# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C0s_2022, y_test_C02022, X_train_C0s_2022, y_train_C02022)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2019 = FindNeuralNetworkParam_GridSearch(X_test_C0s_2022, y_test_C02022, X_train_C0s_2022, y_train_C02022)
results_gs = pd.DataFrame(gridSearch_2019.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', X_test_C0s_2022, y_test_C02022, X_train_C0s_2022, y_train_C02022)
    
Output:
Mean Score for Random Forest: 0.7007
 **Evaluation of Errors for Random Forest Regressor**
R2 score from the Random Forest model: 0.71
Mean Score for RandomForestWithParams: 0.7394
 R2 score from the Random Forest modelWithParams: 0.75
Mean Score for GradientBoosting: 0.6987
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.72
Mean Score for GradientBoostingWithParams: 0.7431
R2 score from the Gradient Boosting modelWithParams 0.76
Mean Score for SVM: 0.1359
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.12
Mean Score for Neural Network: 0.36
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.27
R2 score from the NN model Three Layers: relu 50 40 20 0.70

####################################################################
# 4.2.4.1.2 Feature Selection for cluster 0
####################################################################

####################################################################
# 4.2.4.1.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C02022.feature_importances_
np.sum(importances)
plt.barh(fn_C02022, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C02022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C0_reduced = selector.fit_transform(X_C02022, y_C02022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C02022):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C02022, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

Output: 
** 13 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type', 'View Quality', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Fireplaces', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
 
Mean Score for Random Forest: 0.6864
 **Performance Metrics for Random Forest Regressor**
R2 score from the Random Forest model: 0.69
Mean Score for RandomForestWithParams: 0.7392
 R2 score from the Random Forest modelWithParams: 0.76
####################################################################
# 4.2.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C02022.feature_importances_
np.sum(importances)
plt.barh(fn_C02022, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C02022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C0_reduced = selector.fit_transform(X_C02022, y_C02022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C02022):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C02022, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

Output: 

** 11 features are selected.
Selected features for GBR are:
 ['Land Net Square Feet', 'Waterfront Type', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Fireplaces', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for GradientBoosting: 0.6452
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.68
 
Mean Score for GradientBoostingWithParams: 0.6892
R2 score from the Gradient Boosting modelWithParams 0.70
####################################################################
# 4.2.4.1.3 PCA For Cluster 0
####################################################################

pca_prep = PCA().fit(X_C0s_2022)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 26  
n_pc = 26
pca_C02022 = PCA(n_components = n_pc).fit(X_C0s_2022)
Xp_C02022 = pca_C02022.transform(X_C0s_2022)
print(f'After PCA, we use {pca_C02022.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train_C02022, Xp_test_C02022, yp_train_C02022, yp_test_C02022 = train_test_split(Xp_C02022, y_C02022, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C02022 = RandomForestRegressorModel(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# Grid Seach for best Random Forest Regressor
FindRandomForestRegressorParam_GridSearch(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# For Gradient Boosting 
gbr_C02022 = GradientBoostingRegressorModel(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# Grid Seach for best Gradient Boosting
FindGradientBoostingRegressorParam_GridSearch(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_C02022, yp_test_C02022, Xp_train_C02022, yp_train_C02022)
          
Output:
  
  
Mean Score for Random Forest: 0.6782
 **Performance Metrics for Random Forest Regressor**
R2 score from the Random Forest model: 0.69
Mean Score for RandomForestWithParams: 0.7003
 R2 score from the Random Forest modelWithParams: 0.72
Mean Score for GradientBoosting: 0.7683
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.73
Mean Score for GradientBoostingWithParams: 0.7320
R2 score from the Gradient Boosting modelWithParams 0.75
Mean Score for Neural Network: 0.18
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.16
R2 score from the NN model Three Layers: relu 50 40 20 0.61

####################################################################
# 4.2.4.2 Analyzing Cluster 1
####################################################################

X_C12022 = c1_2022.loc[:, c1_2022.columns != 'Sale Price']
y_C12022 = c1_2022[['Sale Price']].values.ravel()
fn_C12022 = X_C12022.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C12022)
X_C1s_2022 = scaler.transform(X_C12022)


# Divide the dataset into training and testing data.
# Divide the scaled dataset into training and testing data
X_train_C1s_2022, X_test_C1s_2022, y_train_C12022, y_test_C12022 = train_test_split(X_C1s_2022, y_C12022, test_size =.30,random_state=1234) 

# Divide the dataset into training and testing data.
X_train_C12022, X_test_C12022, y_train_C12022, y_test_C12022 = train_test_split(X_C12022, y_C12022, test_size =.30,random_state=1234) 

####################################################################
# 4.2.4.2.1  Building Models With Default Parameters for Cluster 1
####################################################################

# Decision Tree Regressor
dtr_C12022 = DecisionTreeRegressorModel(X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
# Random Forest Regressor
rfr_C12022 = RandomForestRegressorModel(X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
# Gradient Boosting Regression
grbr_C12022 = GradientBoostingRegressorModel(X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_test_C12022, y_test_C12022, X_train_C12022, y_train_C12022)

# SVM Regression
svmr_C12022 = SVMRegressorModel('linear', 10, 0.01, X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)

# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2022 = FindNeuralNetworkParam_GridSearch(X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)
results_gs = pd.DataFrame(gridSearch_2022.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', X_test_C1s_2022, y_test_C12022, X_train_C1s_2022, y_train_C12022)
        

Output:
Mean Score for Decision Tree: 0.8209
**Performance Evaluations for Decision Tree**
R2 score from the Decision Tree model: 0.80
Mean Score for RandomForest: 0.7652
 **Evaluation of Errors for Random Forest Regressor**
R2 score from the Random Forest model: 0.79
Mean Score for RandomForestWithParams: 0.8119
 R2 score from the Random Forest modelWithParams: 0.82
Mean Score for GradientBoosting: 0.7963
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.78
Mean Score for GradientBoostingWithParams: 0.8005
R2 score from the Gradient Boosting modelWithParams 0.81
Mean Score for SVM: 0.0348
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.08
Mean Score for Neural Network: 0.1202
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.11
R2 score from the NN model Three Layers: relu 50 40 20 0.80

####################################################################
# 4.2.4.2.2 Feature Selection for cluster 1
####################################################################

####################################################################
# 4.2.4.2.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C12022.feature_importances_
np.sum(importances)
plt.barh(fn_C12022, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C12022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C1_reduced = selector.fit_transform(X_C12022, y_C12022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C12022):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C12022, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train))

Output:
  

** 11 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Quality', 'Porch Square Feet', 'Attached Garage Square Feet', 'Fireplaces', 'Built-As Square Feet', 'Bedrooms', 'Physical Age', 'HVAC Description_Heat Pump', 'ZipCode', 'Average School Score']
Mean Score for Random Forest: 0.6842
 **Performance Evaluation for Random Forest Regressor**
R2 score from the Random Forest model: 0.68
Mean Score for RandomForestWithParams: 0.7112
 R2 score from the Random Forest modelWithParams: 0.70
####################################################################
# 4.2.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C12022.feature_importances_
np.sum(importances)
plt.barh(fn_C12022, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C12022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C1_reduced = selector.fit_transform(X_C12022, y_C12022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C12022):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C12022, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

Output: 
 

** 8 features are selected.
Selected features for GBR are:
 ['Land Net Square Feet', 'Quality', 'Fireplaces', 'Built-As Square Feet', 'Physical Age', 'HVAC Description_Heat Pump', 'ZipCode', 'Average School Score']
Mean Score for GradientBoosting: 0.6112
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.63
Execution time for GBR: 2.29 seconds
Mean Score for GradientBoostingWithParams: 0.6453
R2 score from the Gradient Boosting modelWithParams 0.65
####################################################################
# 4.2.4.2.3 PCA For Cluster 1
####################################################################

pca_prep = PCA().fit(X_C1s_2022)
pca_prep.n_components_
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 24  
n_pc = 24
pca_C12022 = PCA(n_components = n_pc).fit(X_C1s_2022)
Xp_C12022 = pca_C12022.transform(X_C1s_2022)
print(f'After PCA, we use {pca_C12022.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train_C12022, Xp_test_C12022, yp_train_C12022, yp_test_C12022 = train_test_split(Xp_C12022, y_C12022, test_size =.2, random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C12022 = RandomForestRegressorModel(Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)
# For Gradient Boosting 
gbr_C12022 = GradientBoostingRegressorModel(Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 40, 20,
            'relu', Xp_test_C12022, yp_test_C12022, Xp_train_C12022, yp_train_C12022)      

Output:

  

Mean Score for RandomForest: 0.7105
**Performance Evaluation for Random Forest Regressor**
R2 score from the Random Forest model: 0.72
Mean Score for RandomForestWithParams: 0.7653
 R2 score from the Random Forest modelWithParams: 0.77
Mean Score for GradientBoosting: 0.7002
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.71
Mean Score for GradientBoostingWithParams: 0.7320
R2 score from the Gradient Boosting modelWithParams 0.74
Mean Score for Neural Network: 0.4557
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.38
R2 score from the NN model Three Layers: relu 50 40 20 0.63





































Statement 2 Code

def DecisionTreeRegressorModel(X_test, y_test, X_train, y_train):
    t_start = time.time()
    dtr =  DecisionTreeRegressor(max_leaf_nodes = 500, max_features='auto', splitter='best', random_state = 0)
    dtr_mean_score = np.mean(cross_val_score(dtr, X_train, y_train, cv=5))
    print(f'Mean Score for Decision Tree: {dtr_mean_score:.4f}')
    
    dtr.fit(X_train, y_train)
    y_pred_dtr = dtr.predict(X_test)

    mae = metrics.mean_absolute_error(y_test, y_pred_dtr)
    mse = metrics.mean_squared_error(y_test,y_pred_dtr)
    rmse = np.sqrt(mse)
    print('**Performance Evaluations for Decision Tree**')
    print(f'mae: {mae:.4f}')
    print(f'R2 score from the Decision Tree model: {r2_score(y_test, y_pred_dtr):.2f}')
    print (f'mse: {mse:.4f}')
    print(f'rmse: {rmse:.4f}')
    t_end = time.time()
    print(f'Execution time for DTR: {t_end-t_start:.2f} seconds')
    return dtr
    
def RandomForestRegressorModel(X_test, y_test, X_train, y_train):
    t_start = time.time()
    rfrm = RandomForestRegressor(random_state=1234)
    rfrm_mean_score = np.mean(cross_val_score(rfrm, X_train, y_train, cv=5))
    print(f'Mean Score for RandomForest: {rfrm_mean_score:.4f}')
    
    rfrm.fit(X_train, y_train)
    y_pred_rfrm = rfrm.predict(X_test)
    # Calculate MSE (mean squared errors), RMSE (Root Mean Squred Error),and R^2 for errors.
    mse = metrics.mean_squared_error(y_test,y_pred_rfrm)
    mape = np.mean(np.abs((y_test - y_pred_rfrm)/y_test))*100
    rmse = np.sqrt(mse)

    print('\n', '**Performance Evaluation for Random Forest Regressor**')
    print (' mse: ', mse,'\n','rmse:', rmse)
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the Random Forest model: {r2_score(y_test, y_pred_rfrm):.2f}')
    t_end = time.time()
    print(f'Execution time for RFR: {t_end-t_start:.2f} seconds')
    return rfrm

def RandomForestRegressorModel_WithParams(nEstimator, minSamplesSplit, X_test, y_test, X_train, y_train):
    t_start = time.time()
    rfrm = RandomForestRegressor(n_estimators=nEstimator, min_samples_split=minSamplesSplit, random_state=1234)
    rfrm_mean_score = np.mean(cross_val_score(rfrm, X_train, y_train, cv=5))
    print(f'Mean Score for RandomForest: {rfrm_mean_score:.4f}')
    
    rfrm.fit(X_train, y_train)
    y_pred_rfrm = rfrm.predict(X_test)
    # Calculate MSE (mean squared errors), RMSE (Root Mean Squred Error),and R^2 for errors.
    mse = metrics.mean_squared_error(y_test,y_pred_rfrm)
    mape = np.mean(np.abs((y_test - y_pred_rfrm)/y_test))*100
    rmse = np.sqrt(mse)

    print('\n', '**Performance Evaluation for Random Forest Regressor**')
    print (' mse: ', mse,'\n','rmse:', rmse)
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the Random Forest model: {r2_score(y_test, y_pred_rfrm):.2f}')
    t_end = time.time()
    print(f'Execution time for RFR: {t_end-t_start:.2f} seconds')
    return rfrm

def FindRandomForestRegressorParam_GridSearch(X_test, y_test, X_train, y_train):
    t_start = time.time()
    r2_scorer = make_scorer(r2_score)
    param_grid = {
                'n_estimator':[500, 750, 1000, 1500],
                'min_samples_split': [5, 10, 15, 20], 
             }
    rfrm = RandomForestRegressor(random_state = 1234)
    grid_src = GridSearchCV(estimator= rfrm, param_grid = param_grid, cv=5, scoring=r2_scorer)
    grid_src.fit(X_train,y_train)
    t_end = time.time()
    
    print('\n\n **Report**')
    print("Best Parameters: ", grid_src.best_params_)
    print("Best Mean Test R-squared Score: ", grid_src.best_score_)
    print(f'The best estimator: {grid_src.best_estimator_}')
    print(f'The best parameters:\n {grid_src.best_params_}')
    print(f'The best score: {grid_src.best_score_:.4f}')
    print(f'Total run time for GridSearchCV: {(t_end-t_start):.2f} seconds')
    # Check the details of search
    return pd.DataFrame(grid_src.cv_results_)
    
def GradientBoostingRegressorModel(X_test, y_test, X_train, y_train):
    t_start = time.time()
    gbr = GradientBoostingRegressor()
    gbr_mean_score = np.mean(cross_val_score(gbr, X_train, y_train, cv=5))
    print(f'Mean Score for GradientBoosting: {gbr_mean_score:.4f}')
    
    gbr.fit(X_train, y_train)
    y_pred_gbr = gbr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_gbr)
    mape = np.mean(np.abs((y_test - y_pred_gbr)/y_test))*100
    print(print('**Performance Evaluations for Gradient Boosting**'))
    print(' mse: ', mse,'\n')
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the Gradient Boosting model: {r2_score(y_test, y_pred_gbr):.2f}')
    t_end = time.time()
    print(f'Execution time for GBR: {t_end-t_start:.2f} seconds')
    return gbr

def GradientBoostingRegressorModel_WithParams(nEstimator, minSamplesSplit, X_test, y_test, X_train, y_train):
    t_start = time.time()
    gbr = GradientBoostingRegressor(n_estimators=nEstimator, min_samples_split=minSamplesSplit)
    gbr_mean_score = np.mean(cross_val_score(gbr, X_train, y_train, cv=5))
    print(f'Mean Score for GradientBoosting: {gbr_mean_score:.4f}')
    
    gbr.fit(X_train, y_train)
    y_pred_gbr = gbr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_gbr)
    mape = np.mean(np.abs((y_test - y_pred_gbr)/y_test))*100
    print(print('**Performance Evaluations for Gradient Boosting**'))
    print(' mse: ', mse,'\n')
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the Gradient Boosting model: {r2_score(y_test, y_pred_gbr):.2f}')
    t_end = time.time()
    print(f'Execution time for GBR: {t_end-t_start:.2f} seconds')
    return gbr

def FindGradientBoostingRegressorParam_GridSearch(X_test, y_test, X_train, y_train):
    t_start = time.time()
    r2_scorer = make_scorer(r2_score)
    param_grid = {
                'n_estimator':[500, 750, 1000, 1500],
                'min_samples_split': [5, 10, 15, 20], 
             }
    gbr = GradientBoostingRegressor(random_state = 1234)
    grid_src = GridSearchCV(estimator= gbr, param_grid = param_grid, cv=5, scoring=r2_scorer)
    grid_src.fit(X_train,y_train)
    t_end = time.time()
    
    print('\n\n **Report**')
    print("Best Parameters: ", grid_src.best_params_)
    print("Best Mean Test R-squared Score: ", grid_src.best_score_)
    print(f'The best estimator: {grid_src.best_estimator_}')
    print(f'The best parameters:\n {grid_src.best_params_}')
    print(f'The best score: {grid_src.best_score_:.4f}')
    print(f'Total run time for GridSearchCV: {(t_end-t_start):.2f} seconds')
    # Check the details of search
    return pd.DataFrame(grid_src.cv_results_)
    
def SVMRegressorModel(X_test, y_test, X_train, y_train):
    t_start = time.time()
    svr = SVR(kernel='rbf', C=1.0, epsilon=0.2)
    svr_mean_score = np.mean(cross_val_score(svr, X_train, y_train, cv=5))
    print(f'Mean Score for SVM: {svr_mean_score:.4f}')
    
    # Train the model
    svr.fit(X_train, y_train)
    y_pred_svr = svr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_svr)
    mape = np.mean(np.abs((y_test - y_pred_svr)/y_test))*100
    print('**Performance Evaluations for SVM Regressor**')
    print(' mse: ', mse,'\n')
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the SVM model: {r2_score(y_test, y_pred_svr):.2f}')
    t_end = time.time()
    print(f'Execution time for SVM: {t_end-t_start:.2f} seconds')
    return svr

def NeuralNetworkRegressorModel(X_test, y_test, X_train, y_train):    
    nnr = MLPRegressor(random_state=1234)
    
    nnr_mean_score = np.mean(cross_val_score(nnr, X_train, y_train, cv=5))
    print(f'Mean Score for Neural Network: {nnr_mean_score:.4f}')
    
    nnr.fit(X_train,y_train)
    y_pred_nn = nnr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print(f'R2 score from the NN model: {r2_score(y_test, y_pred_nn):.2f}')
    return nnr


def NeuralNetworkModel_1Layer(hidden_layer_sizes, activationFunc, X_test, y_test, X_train, y_train):
    t_start = time.time()
    nnr = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes),
                       max_iter=3000, activation=activationFunc, random_state=1234)
    nnr.fit(X_train,y_train)
    y_pred_nn = nnr.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    mape = np.mean(np.abs((y_test - y_pred_nn)/y_test))*100
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the NN model One Layer: {activationFunc} {hidden_layer_sizes} {r2_score(y_test, y_pred_nn):.2f}')
    t_end = time.time()
    print(f'Execution time for NNR_1L: {t_end-t_start:.2f} seconds')

def NeuralNetworkModel_2Layer(hidden_layer_sizes, hidden_layer_2, activationFunc, X_test, y_test, X_train, y_train):
    t_start = time.time()
    model = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes, hidden_layer_2), max_iter=1000, 
                         activation=activationFunc, random_state=0)
    model.fit(X_train, y_train)
    y_pred_nn = model.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    mape = np.mean(np.abs((y_test - y_pred_nn)/y_test))*100
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print (f'mape: {mape:.4f}')
    print(f'R2 score from the NN model Two Layers: {activationFunc} {hidden_layer_sizes} {hidden_layer_2} {r2_score(y_test, y_pred_nn):.2f}')
    t_end = time.time()
    print(f'Execution time for NNR_2L: {t_end-t_start:.2f} seconds')

def NeuralNetworkModel_3Layer(hidden_layer_sizes, hidden_layer_2, hidden_layer_3, activationFunc, 
                                  X_test, y_test, X_train, y_train):
    t_start = time.time()
    model = MLPRegressor(hidden_layer_sizes=(hidden_layer_sizes, hidden_layer_2, hidden_layer_3), max_iter=1000, 
                         activation=activationFunc, random_state=0)
    
    nnr_mean_score = np.mean(cross_val_score(model, X_train, y_train, cv=5))
    print(f'Mean Score for Neural Network: {nnr_mean_score:.4f}')
    
    model.fit(X_train, y_train)
    y_pred_nn = model.predict(X_test)
    mse = metrics.mean_squared_error(y_test,y_pred_nn)
    print('**Performance Evaluations for Neural Network**')
    print(' mse: ', mse,'\n')
    print(f'R2 score from the NN model Two Layers: {activationFunc} {hidden_layer_sizes} {hidden_layer_2} {hidden_layer_3} {r2_score(y_test, y_pred_nn):.2f}')
    t_end = time.time()
    print(f'Execution time for NNR_2L: {t_end-t_start:.2f} seconds')
    
def FindNeuralNetworkParam_GridSearch(X_test, y_test, X_train, y_train):
    t_start = time.time()
    r2_scorer = make_scorer(r2_score)
    param_grid = {
                'hidden_layer_sizes':[(20), (30)],
                'activation': ['relu'], 
                'max_iter': [4000,5000]
             }
    nnm_r = MLPRegressor()
    grid_src = GridSearchCV(estimator= nnm_r, param_grid = param_grid, cv=5, scoring=r2_scorer)
    grid_src.fit(X_train,y_train)
    t_end = time.time()
    
    print('\n\n **Report**')
    print("Best Parameters: ", grid_src.best_params_)
    print("Best Mean Test R-squared Score: ", grid_src.best_score_)
    print(f'The best estimator: {grid_src.best_estimator_}')
    print(f'The best parameters:\n {grid_src.best_params_}')
    print(f'The best score: {grid_src.best_score_:.4f}')
    print(f'Total run time for GridSearchCV: {(t_end-t_start):.2f} seconds')
    # Check the details of search
    return pd.DataFrame(grid_src.cv_results_)

def AnalyzingDatasets(df):
    ## show the first five rows of the Appraisal dataset
    print("First 5 rows of the dataset:")
    print(df.head())

    # Check the data types of each column
    print("Data types:")
    print(df.dtypes)

    # Check the number of missing values in each column
    print("Missing values:")
    print(df.isnull().sum())
    df.info()

    # Get the summary statistics of numeric variables (appraisal dataset)
    print("Summary statistics:")
    print(df.describe())

    # Distinct Count & frequency of nominal variables
    df.nunique()

    for colName in df.columns:
        print("UniqueValues for " + colName)
        print(df[colName].value_counts())
    
    #null values present in the Appraisal dataset
    df.isnull().sum().sort_values(ascending=False)

def DetermineCorrelatedVariables(df):
    df_numeric = df.select_dtypes(include=np.number)
    corrMatrix = df_numeric.corr()
    threshold = 0.7
    highly_correlated_variables = []

    # Iterate through each column in the correlation matrix
    for i in range(len(df.columns)):
        for j in range(i + 1, len(corrMatrix.columns)):
            if abs(corrMatrix.iloc[i, j]) >= threshold:
                # Append the names of highly correlated variables to the list
                variable_i = corrMatrix.columns[i]
                variable_j = corrMatrix.columns[j]
                highly_correlated_variables.append((variable_i, variable_j))

    return highly_correlated_variables

    
def CalculateAverageSchoolScore(dfMergedRow, schoolData):
    schoolRadius = 2*5280   # schools in 2 mile (5280 feet) radius
    dfMergedX = dfMergedRow['X']
    dfMergedY = dfMergedRow['Y']
    zipCode = dfMergedRow['ZipCode']
    
    filteredSchoolData = schoolData[(schoolData['X_COORD'] <= dfMergedX + schoolRadius) 
                               & (schoolData['X_COORD'] >= dfMergedX - schoolRadius) 
                               & (schoolData['Y_COORD'] <= dfMergedY + schoolRadius) 
                               & (schoolData['Y_COORD'] >= dfMergedY - schoolRadius)
                               & (schoolData['ZIP'] == zipCode)]
    if len(filteredSchoolData) == 0:
        return 0
    else:
        return filteredSchoolData["score"].mean()    
    
####################################################################
# 1. Reading the Datasets
####################################################################

input_file_appraisal = 'appraisal_account.txt'
column_names_appraisal = ['Parcel Number', 'Appraisal Account Type', 'Business Name', 'Value Area ID',
                 'Land Economic Area', 'Buildings', 'Group Account Number', 
                 'Land Gross Acres', 'Land Net Acres', 'Land Gross Square Feet', 
                 'Land Net Square Feet', 'Land Gross Front Feet', 'Land Width', 'Land Depth', 
                 'Submerged Area Square Feet', 'Appraisal Date', 'Waterfront Type', 
                 'View Quality', 'Utility Electric', 'Utility Sewer', 'Utility Water', 
                 'Street Type', 'Latitude', 'Longitude']

input_file_improvement = 'improvement.txt'
column_names_improvement = ['Parcel Number', 'Building ID', 'Property Type', 'Neighborhood',
                 'Neighborhood Extension', 'Square Feet', 'Net Square Feet', 
                 'Percent Complete', 'Condition', 'Quality', 'Primary Occupancy Code', 
                 'Primary Occupancy Description', 'Mobile Home Serial Number', 'Mobile Home Total Length', 
                 'Mobile Home Make', 'Attic Finished Square Feet', 'Basement Square Feet', 
                 'Basement Finished Square Feet', 'Carport Square Feet', 'Balcony Square Feet', 
                 'Porch Square Feet', 'Attached Garage Square Feet', 'Detached Garage Square Feet', 
                 'Fireplaces', 'Basement Garage Door']

input_file_improvementBuiltAs = 'improvement_builtas.txt'
column_names_improvementBuiltAs = ['Parcel Number', 'Building ID', 'Built-As Number', 'Built-As ID',
                 'Built-As Description', 'Built-As Square Feet', 'HVAC', 'HVAC Description', 
                 'Exterior', 'Interior', 'Stories', 'Story Height', 'Sprinkler Square Feet', 
                 'Roof Cover', 'Bedrooms', 'Bathrooms', 'Units', 'Class Code', 'Class Description', 
                 'Year Built', 'Year Remodeled', 'Adjusted Year Built', 'Physical Age', 
                 'Built-As Length', 'Built-As Width', 'Mobile Home Mode']
                                
input_file_taxAccount = 'tax_account.txt'
column_names_taxAccount = ['Parcel Number', 'Account Type', 'Property Type', 'Site Address',
                 'Use Code', 'Use Description', 'Tax Year - Prior', 'Tax Code Area - Prior Year', 
                 'Exemption Type - Prior Year', 'Current Use Code - Prior Year', 
                 'Land Value - Prior Year', 'Improvement Value - Prior Year', 'Total Market Value - Prior Year', 
                 'Taxable Value - Prior Year', 'Tax Year - Current', 'Tax Code Area - Current Year', 
                 'Exemption Type - Current Year', 'Current Use Code - Current Year', 
                 'Land Value - Current Year', 'Improvement Value - Current Year', 'Total Market Value - Current Year', 
                 'Taxable Value - Current Year', 'Range', 'Township', 'Section', 'Quarter Section',
                  'Subdivision Name', 'Located On Parcel']


# Set the delimiter used in the input file
delimiter = '|'  # Example: Tab-separated values ('\t'), Comma-separated values (','), etc.

# Read the delimited text file into a pandas DataFrame
df_appraisal = pd.read_csv(input_file_appraisal, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_appraisal)
df_improvement = pd.read_csv(input_file_improvement, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_improvement)
df_improvementBuiltAs = pd.read_csv(input_file_improvementBuiltAs, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_improvementBuiltAs)
df_taxAccount = pd.read_csv(input_file_taxAccount, delimiter=delimiter, encoding='ISO-8859-1', names=column_names_taxAccount)

# Read external Dataset files
df_Address_Points = pd.read_csv('Address_Points.csv')
df_school_Data= pd.read_csv("school_by_zipcode.csv")


####################################################################
# 2. Datasets Preprocessing
####################################################################

# Analyzing Datasets
print("Analyzing Appraisal Data Set")
AnalyzingDatasets(df_appraisal)

print("Analyzing Improvement Data Set")
AnalyzingDatasets(df_improvement)

print("Analyzing Improvement BuiltAs Data Set")
AnalyzingDatasets(df_improvementBuiltAs)

print("Analyzing Tax Account Data Set")
AnalyzingDatasets(df_taxAccount)

print("Analyzing AddressPoints Data Set")
AnalyzingDatasets(df_Address_Points)

print("Analyzing School Data Set")
AnalyzingDatasets(df_school_Data)


####################################################################
# 2.1 Appraisal Dataset Preprocessing
####################################################################

# Filtering the dataset to only include Residential Properties
appraisal_dfFiltered = df_appraisal[(df_appraisal['Appraisal Account Type'] == 'Residential')]
appraisal_dfFiltered.info()
appraisal_dfFiltered.isnull().sum().sort_values(ascending=False) 
appraisal_dfFiltered.dtypes

# Dropping the non-required columns which have more than 30% nulls, correlated and not informative
appraisal_dfFiltered.drop(['Appraisal Account Type', 'Value Area ID', 'Business Name', 'Submerged Area Square Feet', 
                           'Group Account Number', 'Land Economic Area', 'Latitude', 
                           'Appraisal Date'] , axis=1, inplace=True)

appraisal_dfFiltered['Waterfront Type'] = appraisal_dfFiltered['Waterfront Type'].apply(lambda x: 1 if not pd.isnull(x) else 0)
appraisal_dfFiltered['View Quality'] = appraisal_dfFiltered['View Quality'].fillna('N/A')

viewQualityOrd = {'View Quality': {'N/A':0, 'View Lim -':1,'View Lim':2,'View Lim +':3, 
                    'View Good' :4,'View Good +':5, 'View Avg':6,'View Avg +':7,
                      'View V-Good':8,'View V-Good +':9}}
appraisal_dfFiltered = appraisal_dfFiltered.replace(viewQualityOrd)

# Converting Parcel number and Appraisal Date Format
appraisal_dfFiltered['Parcel Number']=(appraisal_dfFiltered['Parcel Number']).apply(str)

numericCorrelatedVariables = DetermineCorrelatedVariables(appraisal_dfFiltered)
print(numericCorrelatedVariables)

# Dropping additional variables based on correlation analysis
appraisal_dfFiltered.drop(['Land Depth', 'Land Gross Acres', 
                           'Land Gross Square Feet', 'Land Width','Land Net Acres'] , axis=1, inplace=True)

appraisal_dfFiltered.columns

#Covert nominal variables into dummy
df_appraisalFinal = pd.get_dummies(appraisal_dfFiltered, columns=['Utility Electric', 
                 'Utility Sewer', 'Utility Water', 'Street Type'], drop_first=True)

df_appraisalFinal.info()

# Finding if there are duplicate parcel numbers in appraisal data set
print([item for item, count in collections.Counter(df_appraisalFinal['Parcel Number']).items() if count > 1])


####################################################################
# 2.2 Improvement Dataset Preprocessing
####################################################################

# Filtering datset to consider only residential properties
df_improvement = df_improvement[(df_improvement['Property Type'] == 'Residential')]

# Dropping the non-required columns which have majority of nulls, highly correlated and not informative
df_improvement.drop(['Mobile Home Serial Number', 'Mobile Home Total Length', 
                   'Mobile Home Make', 'Neighborhood', 'Neighborhood Extension', 'Primary Occupancy Code', 
                   'Primary Occupancy Description', 'Property Type' ] , axis=1, inplace=True)

# Populating null values with 0
df_improvement['Attic Finished Square Feet'] = df_improvement['Attic Finished Square Feet'].fillna(0)
df_improvement['Basement Square Feet'] = df_improvement['Basement Square Feet'].fillna(0)
df_improvement['Carport Square Feet'] = df_improvement['Carport Square Feet'].fillna(0)
df_improvement['Balcony Square Feet'] = df_improvement['Balcony Square Feet'].fillna(0)
df_improvement['Porch Square Feet'] = df_improvement['Porch Square Feet'].fillna(0)
df_improvement['Attached Garage Square Feet'] = df_improvement['Attached Garage Square Feet'].fillna(0)
df_improvement['Detached Garage Square Feet'] = df_improvement['Detached Garage Square Feet'].fillna(0)
df_improvement['Fireplaces'] = df_improvement['Fireplaces'].fillna(0)


df_improvement.isnull().sum().sort_values(ascending=False)
df_improvement.info()

# Removing rows with null values
df_improvement =  df_improvement[~df_improvement['Condition'].isnull()]

# Check the data types of each column
print("Data types:")
print(df_improvement.dtypes)

# Converting Parcel number from Integer to String
df_improvement['Parcel Number']=(df_improvement['Parcel Number']).apply(str)

numericCorrelatedVariables = DetermineCorrelatedVariables(df_improvement)
print(numericCorrelatedVariables)

# Converting Ordinal Variables Condition and Quality to Numeric Variables
conditionOrd = {'Condition': {'Uninhabitable':0, 'Extra Poor':1,'Very Poor':2,'Poor':3, 
                      'low':4,'Fair':5, 'Average':6, 'Avg' : 6, 'Avg.':6,'Good':7, 'Excellent':8}}

qualityOrd = {'Quality': {'Low':0, 'Low Plus':1,'Fair':2,'Fair Plus':3, 
                    'Average' :4,'Average Plus':5, 'Good':6,'Good Plus':7,
                      'Very Good':8,'Very Good Plus':9,'Excellent':10}}

df_improvement = df_improvement.replace(conditionOrd)
df_improvement = df_improvement.replace(qualityOrd)

# Dropping the non-required columns highly correlated 
df_improvement.drop(['Basement Garage Door', 'Square Feet', 'Basement Finished Square Feet', 
                   'Net Square Feet' ] , axis=1, inplace=True)

# Finding if there are duplicate parcel numbers in improvement data set
print([item for item, count in collections.Counter(df_improvement['Parcel Number']).items() if count > 1])


####################################################################
# 2.3 ImprovementBuiltAs Dataset Preprocessing
####################################################################

df_improvementBuiltAs.info()
df_improvementBuiltAs.isnull().sum().sort_values(ascending=False)

df_improvementBuiltAs=  df_improvementBuiltAs[df_improvementBuiltAs['Year Built'] >= 2014]
                               
# Converting Parcel number from int to string
df_improvementBuiltAs['Parcel Number']=(df_improvementBuiltAs['Parcel Number']).apply(str)


# Populating Null values for Exterior with Not Applicable
df_improvementBuiltAs['Exterior'] = df_improvementBuiltAs['Exterior'].fillna('Not Applicable')

# Populating Null values for Interior with Not Applicable
df_improvementBuiltAs['Interior'] = df_improvementBuiltAs['Interior'].fillna('Not Applicable')

# Populating Null values for Roof Cover with Not Applicable
df_improvementBuiltAs['Roof Cover'] = df_improvementBuiltAs['Roof Cover'].fillna('Not Applicable')

df_improvementBuiltAs.info()
df_improvementBuiltAs.isnull().sum().sort_values(ascending=False) 
df_improvementBuiltAs.dtypes

# Dropping coulmns with majority of nulls, non-informative $ highly correlated
df_improvementBuiltAs.drop(['Built-As Number', 'Built-As ID','HVAC', 
                    'Mobile Home Mode','Class Description', 'Class Code'] , axis=1, inplace=True)


df_improvementBuiltAs =  df_improvementBuiltAs[~df_improvementBuiltAs['Physical Age'].isnull()]
df_improvementBuiltAs = df_improvementBuiltAs[~df_improvementBuiltAs['Story Height'].isnull()]  

df_improvementBuiltAs.drop(['HVAC Description', 
                 'Exterior', 'Interior', 'Roof Cover'], axis=1, inplace=True)

df_improvementBuiltAs =  df_improvementBuiltAs[~df_improvementBuiltAs['Bedrooms'].isnull()]
df_improvementBuiltAs.drop(['Units'] , axis=1, inplace=True)

df_improvementBuiltAs =  df_improvementBuiltAs[~df_improvementBuiltAs['Stories'].isnull()]
df_improvementBuiltAs =  df_improvementBuiltAs[~df_improvementBuiltAs['Bathrooms'].isnull()]

# Analyzing highly correlated variables
numericCorrelatedVariables = DetermineCorrelatedVariables(df_improvementBuiltAs)
print(numericCorrelatedVariables)

# Dropping coulmnswhich are non informative and highly correlated
df_improvementBuiltAs.drop(['Sprinkler Square Feet', 'Built-As Length', 'Built-As Width', 'Year Built',  
                  'Adjusted Year Built'] , axis=1, inplace=True)

# Finding if there are duplicate parcel numbers in improvement data set
print([item for item, count in collections.Counter(df_improvementBuiltAs['Parcel Number']).items() if count > 1])
df_improvementBuiltAs.isnull().sum().sort_values(ascending=False) 

####################################################################
# 2.4 TaxAccount Dataset Preprocessing
####################################################################

df_taxAccount.info()
df_taxAccount.isnull().sum().sort_values(ascending=False)

print("Data types:")
print(df_taxAccount.dtypes)

# Converting Parcel number from int to string
df_taxAccount['Parcel Number']=(df_taxAccount['Parcel Number']).apply(str)

# selecting the single family dwelling properties only 
df_taxAccount =  df_taxAccount[df_taxAccount['Use Code'] == 1101]

# considering properties only whose prior year market value is > 100K
df_taxAccount =  df_taxAccount[(df_taxAccount['Total Market Value - Current Year'] >= 100000) &
                               (df_taxAccount['Total Market Value - Current Year'] <= 30000000)]


# Analyzing Correlated Varianles
numericCorrelatedVariables = DetermineCorrelatedVariables(df_taxAccount)
print(numericCorrelatedVariables)

# Dropping variables highly correlated, non-informative and not required for analysis
df_taxAccount.drop(['Account Type', 'Property Type', 'Site Address',
                 'Use Code', 'Use Description', 'Tax Year - Prior', 'Tax Code Area - Prior Year', 
                 'Exemption Type - Prior Year', 'Current Use Code - Prior Year', 
                 'Land Value - Prior Year', 'Improvement Value - Prior Year',  
                 'Taxable Value - Prior Year', 'Tax Year - Current', 'Tax Code Area - Current Year', 
                 'Exemption Type - Current Year', 'Current Use Code - Current Year', 
                 'Land Value - Current Year', 'Improvement Value - Current Year','Range', 
                 'Township', 'Section', 'Quarter Section', 'Subdivision Name', 
                 'Located On Parcel', 'Taxable Value - Current Year', 
                 'Total Market Value - Prior Year'] , axis=1, inplace=True)


# Finding if there are duplicate parcel numbers in improvement data set
print([item for item, count in collections.Counter(df_taxAccount['Parcel Number']).items() if count > 1])

####################################################################
# 2.5 Address Points Dataset Preprocessing
####################################################################

df_Address_Points.info()

# Dropping Non required columns
df_Address_Points.drop(['OBJECTID', 'Address', 'Mail_Stop', 'City', 'State', 'Last_Edited', 
                       'Status', 'HouseNumber', 'PrefixDirectional', 'StreetName', 'StreetType',
                       'PostDirectional', 'Jurisdiction', 'AddressID'] , axis=1, inplace=True)

df_Address_Points.isnull().sum().sort_values(ascending=False) 
df_Address_Points =  df_Address_Points[~df_Address_Points['TaxParcelNumber'].isnull()]


print([item for item, count in collections.Counter(df_Address_Points['TaxParcelNumber']).items() if count > 1])
# Removing rows having duplicate Parcel number
df_Address_Points = df_Address_Points.drop_duplicates(subset=['TaxParcelNumber'], keep='last')

####################################################################
# 2.6 School Dataset Preprocessing
####################################################################
df_school_Data.info()

# Dropping Non required columns
df_school_Data.drop(['X', 'Y', 'OBJECTID', 'NAME', 'ADDRESS', 'CITY', 'DISTRICT',
                       'DIST_NO', 'TYPE', 'PHONE', 'WEBSITE', 'PRS_ID', 'GRADE'] , axis=1, inplace=True)

df_school_Data.isnull().sum().sort_values(ascending=False) 

####################################################################
# 3 Merging various datasets
####################################################################

df_appraisalFinal.info()
df_improvement.info()
df_improvementBuiltAs.info()
df_taxAccount.info()

# Merging tax account and appraisal datasets
df_merged = pd.merge(df_appraisalFinal, df_taxAccount, left_on='Parcel Number', right_on='Parcel Number', how='inner')
df_merged.info()
# Checking for Duplicate Parcel Numbers
print([item for item, count in collections.Counter(df_merged['Parcel Number']).items() if count > 1])


# Merging improvement and improvement BuiltAs dataset
# Merging datasets using left join to consider all rows of improvement table
df_improvement_merged = pd.merge(df_improvement, df_improvementBuiltAs, left_on=['Parcel Number', 'Building ID'], 
                                 right_on=['Parcel Number', 'Building ID'], how='left')

# Dropping Building ID after merging as it is not required for further analysis
df_improvement_merged.drop(['Building ID'] , axis=1, inplace=True)

# Finding if there are duplicate parcel numbers in improvement merged data sets, for duplicate values
# select the property row which got remodelled in the last
df_improvement_merged= df_improvement_merged.sort_values(by=['Year Remodeled'], ascending=True)
print([item for item, count in collections.Counter(df_improvement_merged['Parcel Number']).items() if count > 1])
df_improvement_merged = df_improvement_merged.drop_duplicates(subset=['Parcel Number'], keep= 'last')


# Merging the improvement merged data set with the merged dataset
df_merged = pd.merge(df_merged, df_improvement_merged, left_on='Parcel Number', 
                     right_on='Parcel Number', how='inner')

print([item for item, count in collections.Counter(df_merged['Parcel Number']).items() if count > 1])

df_merged.info()

# Finding the correlation matric for the merged dataset for highly correlated variables
numericCorrelatedVariables = DetermineCorrelatedVariables(df_merged)
print(numericCorrelatedVariables)


# Finding if there are duplicate Parcel numbers in merged data set
print([item for item, count in collections.Counter(df_merged['Parcel Number']).items() if count > 1])
df_merged.isnull().sum().sort_values(ascending=False) 

df_merged.drop(['Built-As Description', 'Buildings'] , axis=1, inplace=True)


# Merging the merged dataset with address points to get the property coordinates information
# Merged the datasets using inner join to consider only properties which are present in both
df_merged = pd.merge(df_merged, df_Address_Points, left_on='Parcel Number', right_on='TaxParcelNumber', how='inner')


# Merging with merged dataset with school dataset

# For each property calculate the average school score of the all the schools which are within 2 mile property radius
# and are present in same zipcode
df_merged['Average School Score']  = df_merged.apply(lambda row: CalculateAverageSchoolScore(row, 
                                        df_school_Data), axis = 1)

df_merged.isnull().sum().sort_values(ascending=False) 

# Finding the correlation matric for the merged dataset for highly correlated variables
numericCorrelatedVariables = DetermineCorrelatedVariables(df_merged)
print(numericCorrelatedVariables)

# Dropping Non Required Columns from the merged datasets
df_merged.drop(['Parcel Number', 'Longitude', 'X', 'Y', 'TaxParcelNumber',
                'Utility Electric_POWER INSTALLED', 'Utility Electric_POWER NO - COMMENT', 
                'Utility Sewer_SEWER/SEPTIC INSTALLED', 'Utility Sewer_SEWER/SEPTIC NO', 
                'Utility Water_WATER INSTALLED', 'Utility Water_WATER NO', 'Street Type_STREET NO ROAD', 
                'Street Type_STREET UNPAVED', 'Utility Sewer_SEWER/SEPTIC NO PERC', 
                'Year Remodeled'] , axis=1, inplace=True)

# Removing rows with null values
df_merged = df_merged.dropna(axis=0)

# Analyzing the correlation matrix of the merged datasets
corrMatrixMarketValue = df_merged.corr()
sns.heatmap(corrMatrixMarketValue, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix - Numeric Variables')
plt.show()

df_merged.info()
df_merged.columns

####################################################################
# 4 Building Market Value Prediction Model
####################################################################

# For Final Datasets
X = df_merged.loc[:, df_merged.columns != 'Total Market Value - Current Year']
y = df_merged[['Total Market Value - Current Year']].values.ravel()
fn = X.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X)
X_s = scaler.transform(X)

# Divide the scaled dataset into training and testing data
X_train_s, X_test_s, y_train, y_test = train_test_split(X_s, y, test_size =.30,random_state=1234) 

# Divide the dataset into training and testing data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =.30,random_state=1234) 

# Decision Tree Regressor
dtr = DecisionTreeRegressorModel(X_test, y_test, X_train, y_train)
# Random Forest Regressor
rfr = RandomForestRegressorModel(X_test, y_test, X_train, y_train)
# Grid Seach for best Random Forest Regressor
FindRandomForestRegressorParam_GridSearch(X_test, y_test, X_train, y_train)

# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(750, 10, X_test, y_test, X_train, y_train)

# Gradient Boosting Regression
grbr = GradientBoostingRegressorModel(X_test, y_test, X_train, y_train)

# Grid Seach for best Gradient Boosting
FindGradientBoostingRegressorParam_GridSearch(X_test, y_test, X_train, y_train)

GradientBoostingRegressorModel_WithParams(500, 10, X_test, y_test, X_train, y_train)

# SVM Regression
svmr = SVMRegressorModel(X_test_s, y_test, X_train_s, y_train)
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_s, y_test, X_train_s, y_train)

# Analyzing Neural Network Model with various combinations
'''
for activationFuncName in ['logistic', 'tanh','relu']:
    for hidden_layer in [250,500,750]:
        rSquare = NeuralNetworkModel_1Layer(hidden_layer, activationFuncName, X_test_s, y_test, 
                                                X_train_s, y_train)


# Continuing with Relu as other activation functions are giving negative values
for activationFuncName in ['relu']:
    for hidden_layer in [40, 50,60, 70, 80]:
        rSquare = NeuralNetworkModel_1Layer(hidden_layer, activationFuncName, X_test_s, y_test,
                                               X_train_s, y_train) 
# Two hidden layers
for activationFuncName in ['relu']:
    for hidden_layer in [50]:
        for hidden_layer2 in [60, 70]:
            rSquare = GetNeuralNetworkParams_2Layer(hidden_layer, hidden_layer2, 
                        activationFuncName, X_test_s, y_test, X_train_s, y_train)
        
for activationFuncName in ['relu']:
    for hidden_layer in [20, 30, 40]:
        for hidden_layer2 in [10, 20, 30]:
            rSquare = GetNeuralNetworkParams_2Layer(hidden_layer, hidden_layer2, 
                        activationFuncName , X_test_s, y_test, X_train_s, y_train
   
'''
# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_MarketValue = FindNeuralNetworkParam_GridSearch(X_test_s, y_test, X_train_s, y_train)
results_gs = pd.DataFrame(gridSearch_MarketValue.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', X_test_s, y_test, X_train_s, y_train)

Output: 
Mean Score for Decision Tree: 0.7539
**Performance Evaluations for Decision Tree**
R2 score from the Decision Tree model: 0.72
Mean Score for RandomForest: 0.8540
 R2 score from the Random Forest model: 0.82
Mean Score for RandomForestWithParams: 0.8529
 R2 score from the Random Forest modelWithParams: 0.86
Mean Score for GradientBoosting: 0.8425
R2 score from the Gradient Boosting model: 0.83
Mean Score for GradientBoostingWithParams: 0.8320
R2 score from the Gradient Boosting modelWithParams 0.85
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: -0.02
Mean Score for Neural Network: 0.2688
R2 score from the NN model: 0.23
**Performance Evaluations for Neural Network**
R2 score from the NN model: 0.80
The best estimators: MLPClassifier(hidden_layer_sizes=(50, 40, 20))
The best parameters for Layers:
 {'activation': 'relu', 'hidden_layer_sizes': (50, 50, 20)}
The best score for Layers: 0.81

####################################################################
# 4.1.2 Feature Selection 
####################################################################

####################################################################
# 4.1.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr.feature_importances_
np.sum(importances)
plt.barh(fn, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(12)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)
# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold=0.015)
X_reduced = selector.fit_transform(X, y)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test \
       = train_test_split(X_reduced, y, test_size =.3, random_state=1234)
RandomForestRegressorModel(X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)
# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

Output: 

 
** 12 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type', 'View Quality', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for RandomForest: 0.8455
R2 score from the Random Forest model: 0.86
Mean Score for RandomForestWithParams: 0.8621
R2 score from the Random Forest modelWithParams: 0.86

####################################################################
# 4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr.feature_importances_
np.sum(importances)
plt.barh(fn, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_reduced = selector.fit_transform(X, y)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_reduced_train, X_reduced_test, y_reduced_train, y_reduced_test \
       = train_test_split(X_reduced, y, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

# With hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, X_reduced_test, y_reduced_test, X_reduced_train, y_reduced_train)

Output: 
 

** 11 features are selected.
Selected features for GBR are:
 ['Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type', 'View Quality', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for GradientBoosting: 0.8368
**Performance Evaluations for Gradient Boosting**
R2 score from the Gradient Boosting model: 0.83
Mean Score for GradientBoostingWithParams: 0.8678
R2 score from the GradientBoostinglWithParams: 0.84

####################################################################
# 4.1.3 PCA
####################################################################

# Create an instance PCA and build the model using Xn.
# We start from the same number of components as the number of original features.
pca_prep = PCA().fit(X_s)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.title('Scree Plot Market Value Data')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 16 
n_pc = 15
pca = PCA(n_components = n_pc).fit(X_s)
Xp = pca.transform(X_s)
print(f'After PCA, we use {pca.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, y, test_size =.3, random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_pca = RandomForestRegressorModel(Xp_test, yp_test, Xp_train, yp_train)
# With Hyperparameters
RandomForestModel_WithParams(750, 10, Xp_test, yp_test, Xp_train, yp_train)

# For Gradient Boosting
gbr_pca = GradientBoostingRegressorModel(Xp_test, yp_test, Xp_train, yp_train)
# With Hyperparameters
GradientBoostingModel_WithParams(750, 10, Xp_test, yp_test, Xp_train, yp_train)

# SVM Regression
svmr_2019 = SVMRegressorModel('linear', 10, 0.01, Xp_test, yp_test, Xp_train, yp_train)

# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test, yp_test, Xp_train, yp_train)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', Xp_test, yp_test,  Xp_train, yp_train)

Output: 
 

Mean Score for RandomForest: 0.8137
R2 score from the Random Forest model: 0.81
Mean Score for RandomForestWithParams: 0.8923
R2 score from the Random Forest modelWithParams: 0.82
Mean Score for GradientBoosting: 0.7461
R2 score from the Gradient Boosting model: 0.72
Mean Score for GradientBoostingWithParams: 0.7099
R2 score from the Gradient Boosting modelWithParams 0.75
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: -0.03
Mean Score for Neural Network: 0.1688
R2 score from the NN model: 0.23
Mean Score for Neural Network: 0.7888
R2 score from the NN model Three Layers: relu 50 50 20 0.78
####################################################################
# 4.1.4 Clustering
####################################################################

n_pc = 2
pca_K = PCA(n_components = n_pc).fit(X_s)
Xp_3 = pca_K.transform(X_s)

# Create an instance (object) of the KMeans class with the parameters
# initialized (cluster count 2)
km = KMeans(n_clusters=2, random_state=1234)
# Build a model.
km = km.fit_predict(Xp_3)

silhouette_avg = silhouette_score(Xp_3, km)
print('Silhouette Score:', silhouette_avg)

c0 = df_merged[km == 0]
c1 = df_merged[km == 1]

c0.shape 
c1.shape 

#Cluster Plot
plt.scatter(Xp_3[:, 0], Xp_3[:, 1], c=km, s=50, cmap='viridis')
plt.title("Scatter Plot with Clusters (Market Value)")
plt.show()

Output:

  
Silhouette Score: 0.48540732684901117

####################################################################
# 4.1.4.1 Analyzing Cluster 0
####################################################################

X_C0 = c0.loc[:, c0.columns != 'Total Market Value - Current Year']
y_C0 = c0[['Total Market Value - Current Year']].values.ravel()
fn_C0 = X_C0.columns


#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C0)
X_C0s = scaler.transform(X_C0)

####################################################################
# 4.1.4.1.1  Building Models Wth Default Parameters for Cluster 0
####################################################################

# Divide the scaled dataset into training and testing data
X_train_C0s, X_test_C0s, y_train_C0, y_test_C0 = train_test_split(X_C0s, y_C0, test_size =.30,random_state=1234) 

# Divide the dataset into training and testing data.
X_train_C0, X_test_C0, y_train_C0, y_test_C0 = train_test_split(X_C0, y_C0, test_size =.30,random_state=1234) 

# Decision Tree Regressor
dtr_C0 = DecisionTreeRegressorModel(X_test_C0, y_test_C0, X_train_C0, y_train_C0)
# Random Forest Regressor
rfr_C0 = RandomForestRegressorModel(X_test_C0, y_test_C0, X_train_C0, y_train_C0)
# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_test_C0, y_test_C0, X_train_C0, y_train_C0)

# Gradient Boosting Regression
grbr_C0 = GradientBoostingRegressorModel(X_test_C0, y_test_C0, X_train_C0, y_train_C0)

GradientBoostingRegressorModel_WithParams(500, 10, X_test_C0, y_test_C0, X_train_C0, y_train_C0)
                
# SVM Regression
svmr_C0 = SVMRegressorModel(X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)
# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch = FindNeuralNetworkParam_GridSearch(X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)
results_gs = pd.DataFrame(gridSearch.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)

Output: 
Mean Score for DecisionTree: 0.8137
R2 score from the DecisionTree model: 0.81
Mean Score for RandomForest: 0.8137
R2 score from the Random Forest model: 0.81
Mean Score for RandomForestWithParams: 0.8923
R2 score from the Random Forest modelWithParams: 0.90
Mean Score for GradientBoosting: 0.8431
R2 score from the Gradient Boosting model: 0.83
Mean Score for GradientBoostingWithParams: 0.8566
R2 score from the Gradient Boosting modelWithParams 0.87
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.05
Mean Score for Neural Network: 0.4398
R2 score from the NN model: 0.53
Mean Score for Neural Network: 0.7888
R2 score from the NN model Three Layers: relu 50 50 20 0.81
####################################################################
# 4.1.4.1.2 Feature Selection for cluster 0
####################################################################

####################################################################
# 4.1.4.1.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C0.feature_importances_
np.sum(importances)
plt.barh(fn_C0, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C0, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C0_reduced = selector.fit_transform(X_C0, y_C0)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C0):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C0, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

Output: 
 
** 12 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type', 'View Quality', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for RandomForest: 0.6879
R2 score from the Random Forest model: 0.69
Mean Score for RandomForestWithParams: 0.7388
R2 score from the Random Forest modelWithParams: 0.70

####################################################################
# 4.1.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C0.feature_importances_
np.sum(importances)
plt.barh(fn_C0, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C0, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C1_reduced = selector.fit_transform(X_C0, y_C0)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C0):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C0, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

#With hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

Output:
 

Selected features for GBR are:
 ['Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type', 'View Quality', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Built-As Square Feet', 'Physical Age', 'ZipCode', 'Average School Score']

Mean Score for GradientBoosting: 0.8853
R2 score from the GradientBoosting model: 0.82
Mean Score for GradientBoostingWithParams: 0.8972
R2 score from the GradientBoostingmodelWithParams: 0.87

####################################################################
# 4.1.4.1.3 PCA For Cluster 0
####################################################################

pca_prep = PCA().fit(X_C0s)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.title('Scree Plot Cluster1: Market Value')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 20  
n_pc = 20
pca_C0 = PCA(n_components = n_pc).fit(X_C0s)
Xp_C0 = pca_C0.transform(X_C0s)
print(f'After PCA, we use {pca_C0.n_components_} components.\n')

# Split the data into training and testing subsets.
Xp_train_C0, Xp_test_C0, yp_train_C0, yp_test_C0 = train_test_split(Xp_C0, y_C0, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C0 = RandomForestRegressorModel(Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# For Gradient Boosting 
gbr_C0 = GradientBoostingRegressorModel(Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# SVM Regression
svmr_C02019 = SVMRegressorModel('linear', 10, 0.01, Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

Output: 
 

Mean Score for RandomForest: 0.8073
R2 score from the Random Forest model: 0.81
Mean Score for RandomForestWithParams: 0.8543
R2 score from the Random Forest modelWithParams: 0.87
Mean Score for GradientBoosting: 0.8231
R2 score from the Gradient Boosting model: 0.83
Mean Score for GradientBoostingWithParams: 0.7985
R2 score from the Gradient Boosting modelWithParams 0.82
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.06
Mean Score for Neural Network: 0.8345
R2 score from the NN model: 0.80
Mean Score for Neural Network: 0.8103
R2 score from the NN model Three Layers: relu 50 50 20 0.84

####################################################################
# 4.1.4.2 Analyzing Cluster 1
####################################################################

X_C1 = c1.loc[:, c1.columns != 'Total Market Value - Current Year']
y_C1 = c1[['Total Market Value - Current Year']].values.ravel()
fn_C1 = X_C1.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C1)
X_C1s = scaler.transform(X_C1)

# Divide the dataset into training and testing data.
# Divide the scaled dataset into training and testing data
X_train_C1s, X_test_C1s, y_train_C1, y_test_C1 = train_test_split(X_C1s, y_C1, test_size =.30,random_state=1234) 
# Divide the dataset into training and testing data.
X_train_C1, X_test_C1, y_train_C1, y_test_C1 = train_test_split(X_C1, y_C1, test_size =.30,random_state=1234) 

####################################################################
# 4.1.4.2.1  Building Models Wth Default Parameters for Cluster 1
####################################################################

# Decision Tree Regressor
dtr_C1 = DecisionTreeRegressorModel(X_test_C1, y_test_C1, X_train_C1, y_train_C1)
# Random Forest Regressor
rfr_C1 = RandomForestRegressorModel(X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# Gradient Boosting Regression
grbr_C1 = GradientBoostingRegressorModel(X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# SVM Regression
svmr_C1 = SVMRegressorModel(X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)
# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2019 = FindNeuralNetworkParam_GridSearch(X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)
results_gs = pd.DataFrame(gridSearch_2019.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)

Output: 

Mean Score for RandomForest: 0.7709
R2 score from the Random Forest model: 0.78
Mean Score for RandomForestWithParams: 0.7983
R2 score from the Random Forest modelWithParams: 0.82
Mean Score for GradientBoosting: 0.7786
R2 score from the Gradient Boosting model: 0.78
Mean Score for GradientBoostingWithParams: 0.7654
R2 score from the Gradient Boosting modelWithParams 0.79
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.02
Mean Score for Neural Network: 0.1809
R2 score from the NN model: 0.17
Mean Score for Neural Network: 0.4987
R2 score from the NN model Three Layers: relu 50 50 20 0.51

####################################################################
# 4.2.4.2.2 Feature Selection for cluster 1
####################################################################

####################################################################
# 4.2.4.2.2.1 Feature Selection Using Random Forest Regressor
####################################################################


# Find the significant features for Pierce County with their importance values.
importances = rfr_C1.feature_importances_
np.sum(importances)
plt.barh(fn_C1, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C1, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C1_reduced = selector.fit_transform(X_C1, y_C1)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C1):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C1, test_size =.3, random_state=1234)

# With Default 
RandomForestRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_test, y_test, X_train, y_train)

Output: 
 
** 14 features are selected.
Selected features for Random Forest Regressor are:
 ['Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type', 'Percent Complete', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Bedrooms', 'Bathrooms', 'Physical Age', 'ZipCode', 'Average School Score']

Mean Score for RandomForest: 0.6476
R2 score from the Random Forest model: 0.62
Mean Score for RandomForestWithParams: 0.6632
R2 score from the Random Forest modelWithParams: 0.67

####################################################################
# 4.2.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C1.feature_importances_
np.sum(importances)
plt.barh(fn_C1, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C1, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C1_reduced = selector.fit_transform(X_C1, y_C1)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C1):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C1, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

Output:
 
 
** 15 features are selected.
Selected features for GBR are:
 ['Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type', 'View Quality', 'Percent Complete', 'Quality', 'Basement Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Built-As Square Feet', 'Bedrooms', 'Bathrooms', 'Physical Age', 'ZipCode', 'Average School Score']
Mean Score for RandomForest: 0.7649
R2 score from the Random Forest model: 0.79
Mean Score for RandomForestWithParams: 0.8369
R2 score from the Random Forest modelWithParams: 0.86

####################################################################
# 4.1.4.2.3 PCA For Cluster 1
####################################################################

pca_prep = PCA().fit(X_C1s)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 15 
n_pc = 16
pca_C1 = PCA(n_components = n_pc).fit(X_C1s)
Xp_C1 = pca_C1.transform(X_C1s)
print(f'After PCA, we use {pca_C1.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train_C1, Xp_test_C1, yp_train_C1, yp_test_C1 = train_test_split(Xp_C1, y_C1, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C1 = RandomForestRegressorModel(Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# For Gradient Boosting 
gbr_C1 = GradientBoostingRegressorModel(Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)     

Output:
  

Mean Score for RandomForest: 0.6835
R2 score from the Random Forest model: 0.70
Mean Score for RandomForestWithParams: 0.7109
R2 score from the Random Forest modelWithParams: 0.72
Mean Score for GradientBoosting: 0.6548
R2 score from the Gradient Boosting model: 0.66
Mean Score for GradientBoostingWithParams: 0.7154
R2 score from the Gradient Boosting modelWithParams 0.73
**Performance Evaluations for SVM Regressor**
R2 score from the SVM model: 0.06
Mean Score for Neural Network: 0.1265
R2 score from the NN model: 0.11
Mean Score for Neural Network: 0.5986
R2 score from the NN model Three Layers: relu 50 50 20 0.68




 
Table #. Highly Correlated Attributes
table	attributes	 corr 
appraisal	Land Gross Acres  &  Land Net Acres 	        0.997 
	Land Gross Acres  &  Land Gross Square Feet 	        1.000 
	Land Gross Acres  &  Land Net Square Feet 	        0.997 
	Land Net Acres  &  Land Gross Square Feet 	        0.997 
	Land Net Acres  &  Land Net Square Feet 	        1.000 
	Land Gross Square Feet  &  Land Net Square Feet 	        0.997 
improvement	Mobile Home Total Length  &  Detached Garage Square Feet 	-      1.000 
	Basement Square Feet  &  Basement Finished Square Feet 	        0.752 
improvementBuiltAs	Built-As Square Feet  &  Sprinkler Square Feet 	        0.915 
	Bathrooms  &  Bedrooms 	        0.855 
	Year Built  &  Physical Age 	-      0.762 
	Built-As Width  &  Built-As Length 	        0.949 
sale	Parcel Count  &  Sale Price 	        0.752 
tax	Tax Code Area-Prior Year  &  Tax Code Area-Current Year 	        1.000 
	Land Value-Prior Year  &  Total Market Value-Prior Year 	        0.782 
	Land Value-Prior Year  &  Land Value-Current Year 	        0.988 
	Land Value-Prior Year  &  Total Market Value-Current Year 	        0.757 
	Improvement Value-Prior Year  &  Total Market Value-Prior Year 	        0.952 
	Improvement Value-Prior Year  &  Taxable Value-Prior Year 	        0.789 
	Improvement Value-Prior Year  &  Improvement Value-Current Year 	        0.993 
	Improvement Value-Prior Year  &  Total Market Value-Current Year 	        0.941 
	Improvement Value-Prior Year  &  Taxable Value-Current Year 	        0.712 
	Total Market Value-Prior Year  &  Taxable Value-Prior Year 	        0.809 
	Total Market Value-Prior Year  &  Land Value-Current Year 	        0.775 
	Total Market Value-Prior Year  &  Improvement Value-Current Year 	        0.938 
	Total Market Value-Prior Year  &  Total Market Value-Current Year 	        0.995 
	Total Market Value-Prior Year  &  Taxable Value-Current Year 	        0.743 
	Taxable Value-Prior Year  &  Total Market Value-Current Year 	        0.734 
	Taxable Value-Prior Year  &  Taxable Value-Current Year 	        0.993 
	Land Value-Current Year  &  Total Market Value-Current Year 	        0.772 
	Improvement Value-Current Year  &  Total Market Value-Current Year 	        0.947 
	Improvement Value-Current Year  &  Taxable Value-Current Year 	        0.706 
	Total Market Value-Current Year  &  Improvement Value-Current Year 	        0.947 
	Total Market Value-Current Year  &  Taxable Value-Current Year 	        0.740 
	Taxable Value-Current Year  &  Improvement Value-Current Year 	        0.706

Table#. Null value
table	Columns with more than 30% of missing value	Columns with 10~30% of missing value	Columns with less than 10% of missing value
appraisal_account	Business_Name 95.92
Group_Account_Number 94.66
Submerged_Area_Square_Feet 83.91		Land_Economic_Area 0.01 %
Buildings 0.03 %
Land_Gross_Acres 0.0 %
Land_Net_Acres 4.97 %
Land_Net_Square_Feet 4.97 %
Land_Width 3.71 %
Appraisal_Date 0.16 %Latitude 0.35 %
Longitude 0.35 %
improvement			Neighborhood 0.01 %
Neighborhood_Extension 0.01 %
Net_Square_Feet 0.03 %
Condition 0.05 %
Quality 0.05 %
improvement_builtas	HVAC 100.0
Class_Code 87.23
Class_Description 87.23
Mobile_Home_Model 93.61	Exterior
Interior
Roof_Cover	HVAC_Description 5.21 %
Stories 0.39 %
Sprinkler_Square_Feet 0.05 %
Bedrooms 8.05 %
Bathrooms 8.02 %
Units 1.59 %
Built-As_Length 2.24 %
Built-As_Width 2.24 %
sale	-	-	Grantor 2.02 %
Grantee 1.97 %
Tax_account	-	-	Property_Type 3.03 %
Use_Code 0.19 %
Use_Description 0.19 %Tax_Code_Area-Prior_Year 0.13 %
Land_Value-Prior_Year 3.45 %
Improvement_Value-Prior_Year 3.09 %
Total_Market_Value-Prior_Year 0.14 %
Taxable_Value-Prior_Year 0.14 %
Tax_Code_Area-Current_Year 3.34 %
Land_Value-Current_Year 3.45 %
Improvement_Value-Current_Year 3.45 %
Total_Market_Value-Current_Year 3.44 %
Taxable_Value-Current_Year 3.44 %
Range 6.82 %
Township 6.82 %
Section 6.82 %
Quarter_Section 6.82 %
School score	-	-	

Table #. nominal attributes
Table	Nominal attributes
Appraisal account	Appraisal_Account_Type
Value_Area_ID
Utility_Water
Utility_Electric
Utility_Sewer
Street_Type
Land_Economic_Area

improvement	Property_Type
Neighborhood
Neighborhood_Extension
Primary_Occupancy_Code
Primary_Occupancy_Description
improvement_builtas	Built-As_Number
Interior
Exterior
Roof_Cover
Parcel_Number
Building_ID
sale	Deed_Type
Appraisal_Account_Type
Grantor
Grantee
tax_account	Account_Type
Property_Type
Site_Address
Use_Code
Tax_Code_Area-Prior_Year
Tax_Code_Area-Current_Year
Range
Township
Section
Quarter_Section


Table#. Ordinal Attributes
Table	Ordinal attributes
	View Quality
Quality
Condition


Table#. Date attributes

Table	Date type attributes
Appraisal_account	Appraisal_Date
Sale	Sale_Date
Improvement_builtas	Year_Built
Year_Remodeled
Adjusted_Year_Built
Tax_account	Tax_Year-Prior
Tax_Year-Current


   

Variables for Sales 2019 and 2022 Data: Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type’, 'View Quality', 'Sale Price', 'Condition', 'Quality’, 'Attic Finished Square Feet', 'Basement Square Feet’, 'Carport Square Feet', 'Balcony Square Feet', 'Porch Square Feet’, 'Attached Garage Square Feet', 'Detached Garage Square Feet’, 'Fireplaces', 'Built-As Square Feet', 'Stories', 'Bedrooms’, 'Physical Age', 'HVAC Description, 'ZipCode’, 'Average School Score'.
Variables for Market Value prediction Data: 'Land Net Square Feet', 'Land Gross Front Feet', 'Waterfront Type, 'View Quality', 'Total Market Value - Current Year', 'Percent Complete', 'Condition', 'Quality', 'Attic Finished Square Feet', 'Basement Square Feet', 'Carport Square Feet', 'Balcony Square Feet', 'Porch Square Feet', 'Attached Garage Square Feet', 'Detached Garage Square Feet', 'Fireplaces', 'Built-As Square Feet', 'Stories', 'Story Height', 'Bedrooms', 'Bathrooms', 'Physical Age', 'ZipCode', 'Average School Score'
