####################################################################
# 4.2.4.1.2 Feature Selection for cluster 0
####################################################################

####################################################################
# 4.2.4.1.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C02022.feature_importances_
np.sum(importances)
plt.barh(fn_C02022, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C02022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C0_reduced = selector.fit_transform(X_C02022, y_C02022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C02022):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C02022, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)
# With Hyperparameters for Random Forest
RandomForestRegressorModel_WithParams(1000, 15, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

####################################################################
# 4.2.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C02022.feature_importances_
np.sum(importances)
plt.barh(fn_C02022, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C02022, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C0_reduced = selector.fit_transform(X_C02022, y_C02022)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C02022):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C02022, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)
#With Hyperparameters for Gradient Boosting
GradientBoostingRegressorModel_WithParams(750, 10, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)
