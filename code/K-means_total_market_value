####################################################################
# 4.1.4 Clustering
####################################################################

n_pc = 2
pca_K = PCA(n_components = n_pc).fit(X_s)
Xp_3 = pca_K.transform(X_s)

# Create an instance (object) of the KMeans class with the parameters
# initialized (cluster count 2)
km = KMeans(n_clusters=2, random_state=1234)
# Build a model.
km = km.fit_predict(Xp_3)

silhouette_avg = silhouette_score(Xp_3, km)
print('Silhouette Score:', silhouette_avg)

c0 = df_merged[km == 0]
c1 = df_merged[km == 1]

c0.shape 
c1.shape 

#Cluster Plot
plt.scatter(Xp_3[:, 0], Xp_3[:, 1], c=km, s=50, cmap='viridis')
plt.title("Scatter Plot with Clusters (Market Value)")
plt.show()

####################################################################
# 4.1.4.1 Analyzing Cluster 0
####################################################################

X_C0 = c0.loc[:, c0.columns != 'Total Market Value - Current Year']
y_C0 = c0[['Total Market Value - Current Year']].values.ravel()
fn_C0 = X_C0.columns


#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C0)
X_C0s = scaler.transform(X_C0)

####################################################################
# 4.1.4.1.1  Building Models Wth Default Parameters for Cluster 0
####################################################################

# Divide the scaled dataset into training and testing data
X_train_C0s, X_test_C0s, y_train_C0, y_test_C0 = train_test_split(X_C0s, y_C0, test_size =.30,random_state=1234) 

# Divide the dataset into training and testing data.
X_train_C0, X_test_C0, y_train_C0, y_test_C0 = train_test_split(X_C0, y_C0, test_size =.30,random_state=1234) 

# Decision Tree Regressor
dtr_C0 = DecisionTreeRegressorModel(X_test_C0, y_test_C0, X_train_C0, y_train_C0)
# Random Forest Regressor
rfr_C0 = RandomForestRegressorModel(X_test_C0, y_test_C0, X_train_C0, y_train_C0)
# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_test_C0, y_test_C0, X_train_C0, y_train_C0)

# Gradient Boosting Regression
grbr_C0 = GradientBoostingRegressorModel(X_test_C0, y_test_C0, X_train_C0, y_train_C0)

GradientBoostingRegressorModel_WithParams(500, 10, X_test_C0, y_test_C0, X_train_C0, y_train_C0)
                
# SVM Regression
svmr_C0 = SVMRegressorModel(X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)
# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch = FindNeuralNetworkParam_GridSearch(X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)
results_gs = pd.DataFrame(gridSearch.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', X_test_C0s, y_test_C0, X_train_C0s, y_train_C0)


####################################################################
# 4.1.4.1.2 Feature Selection for cluster 0
####################################################################

####################################################################
# 4.1.4.1.2.1 Feature Selection Using Random Forest Regressor
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = rfr_C0.feature_importances_
np.sum(importances)
plt.barh(fn_C0, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C0, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C0_reduced = selector.fit_transform(X_C0, y_C0)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C0):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C0, test_size =.3, random_state=1234)

RandomForestRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

####################################################################
# 4.1.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C0.feature_importances_
np.sum(importances)
plt.barh(fn_C0, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C0, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C1_reduced = selector.fit_transform(X_C0, y_C0)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C0):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C0_reduced_train, X_C0_reduced_test, y_C0_reduced_train, y_C0_reduced_test \
       = train_test_split(X_C0_reduced, y_C0, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

#With hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, X_C0_reduced_test, y_C0_reduced_test, X_C0_reduced_train, y_C0_reduced_train)

####################################################################
# 4.1.4.1.3 PCA For Cluster 0
####################################################################

pca_prep = PCA().fit(X_C0s)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.title('Scree Plot Cluster1: Market Value')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 20  
n_pc = 20
pca_C0 = PCA(n_components = n_pc).fit(X_C0s)
Xp_C0 = pca_C0.transform(X_C0s)
print(f'After PCA, we use {pca_C0.n_components_} components.\n')

# Split the data into training and testing subsets.
Xp_train_C0, Xp_test_C0, yp_train_C0, yp_test_C0 = train_test_split(Xp_C0, y_C0, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C0 = RandomForestRegressorModel(Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# For Gradient Boosting 
gbr_C0 = GradientBoostingRegressorModel(Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# SVM Regression
svmr_C02019 = SVMRegressorModel('linear', 10, 0.01, Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', Xp_test_C0, yp_test_C0, Xp_train_C0, yp_train_C0)

####################################################################
# 4.1.4.2 Analyzing Cluster 1
####################################################################

X_C1 = c1.loc[:, c1.columns != 'Total Market Value - Current Year']
y_C1 = c1[['Total Market Value - Current Year']].values.ravel()
fn_C1 = X_C1.columns

#Scaling the variables
scaler = StandardScaler()
scaler.fit(X_C1)
X_C1s = scaler.transform(X_C1)

# Divide the dataset into training and testing data.
# Divide the scaled dataset into training and testing data
X_train_C1s, X_test_C1s, y_train_C1, y_test_C1 = train_test_split(X_C1s, y_C1, test_size =.30,random_state=1234) 
# Divide the dataset into training and testing data.
X_train_C1, X_test_C1, y_train_C1, y_test_C1 = train_test_split(X_C1, y_C1, test_size =.30,random_state=1234) 

####################################################################
# 4.1.4.2.1  Building Models Wth Default Parameters for Cluster 1
####################################################################

# Decision Tree Regressor
dtr_C1 = DecisionTreeRegressorModel(X_test_C1, y_test_C1, X_train_C1, y_train_C1)
# Random Forest Regressor
rfr_C1 = RandomForestRegressorModel(X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# Gradient Boosting Regression
grbr_C1 = GradientBoostingRegressorModel(X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, X_test_C1, y_test_C1, X_train_C1, y_train_C1)

# SVM Regression
svmr_C1 = SVMRegressorModel(X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)
# NN Regression
# Neural network with default parameters
NeuralNetworkRegressorModel(X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)

# Optimizing the NN Model using Hyper Parameters Tuning Using Grid Search
gridSearch_2019 = FindNeuralNetworkParam_GridSearch(X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)
results_gs = pd.DataFrame(gridSearch_2019.cv_results_)

# Evaluating Neural network results with grid search best estimators values
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', X_test_C1s, y_test_C1, X_train_C1s, y_train_C1)


####################################################################
# 4.2.4.2.2 Feature Selection for cluster 1
####################################################################

####################################################################
# 4.2.4.2.2.1 Feature Selection Using Random Forest Regressor
####################################################################


# Find the significant features for Pierce County with their importance values.
importances = rfr_C1.feature_importances_
np.sum(importances)
plt.barh(fn_C1, importances)

# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C1, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
df_importance_top15 = df_importances.head(11)
plt.barh(df_importance_top15.index, df_importance_top15.importance_value)
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=RandomForestRegressor(), threshold= 0.015)
X_C1_reduced = selector.fit_transform(X_C1, y_C1)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C1):
    if i: selected_features.append(j)
print(f'Selected features for Random Forest Regressor are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C1, test_size =.3, random_state=1234)

# With Default 
RandomForestRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, X_test, y_test, X_train, y_train)


####################################################################
# 4.2.4.1.2.2 Feature Selection Using Gradient Boosting
####################################################################

# Find the significant features for Pierce County with their importance values.
importances = grbr_C1.feature_importances_
np.sum(importances)
plt.barh(fn_C1, importances)
# Draw a bar chart to see the sorted importance values with feature names.
df_importances = pd.DataFrame(data=importances, index=fn_C1, columns=['importance_value'])
df_importances.sort_values(by = 'importance_value', ascending=False, inplace=True)
df_importances
plt.barh(df_importances.index, df_importances.importance_value)

# Build a model with a subset of those features.   
selector = SelectFromModel(estimator=GradientBoostingRegressor(), threshold=0.015)
X_C1_reduced = selector.fit_transform(X_C1, y_C1)
selector.threshold_ 
selected_TF = selector.get_support()
print(f'\n** {selected_TF.sum()} features are selected.')  

# Show those selected features.
selected_features = []
for i,j in zip(selected_TF, fn_C1):
    if i: selected_features.append(j)
print(f'Selected features for GBR are:\n {selected_features}')

# Build a model using reduced number of features
X_C1_reduced_train, X_C1_reduced_test, y_C1_reduced_train, y_C1_reduced_test \
       = train_test_split(X_C1_reduced, y_C1, test_size =.3, random_state=1234)

GradientBoostingRegressorModel(X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, X_C1_reduced_test, y_C1_reduced_test, X_C1_reduced_train, y_C1_reduced_train)

####################################################################
# 4.1.4.2.3 PCA For Cluster 1
####################################################################

pca_prep = PCA().fit(X_C1s)
pca_prep.n_components_

# Consider the variances as the amount of information.  Drop components providing less information
# (low variances)
pca_prep.explained_variance_ratio_

# Create a scree plot and find an "elbow" or an inflection point on the plot.
plt.plot(pca_prep.explained_variance_ratio_)
plt.xlabel('k number of components')
plt.ylabel('Explained variance')
plt.grid(True)
plt.show()

# From the scree plot, we choose k = 15 
n_pc = 16
pca_C1 = PCA(n_components = n_pc).fit(X_C1s)
Xp_C1 = pca_C1.transform(X_C1s)
print(f'After PCA, we use {pca_C1.n_components_} components.\n')

# Split the data into training and testing subsets.

Xp_train_C1, Xp_test_C1, yp_train_C1, yp_test_C1 = train_test_split(Xp_C1, y_C1, test_size =.2, 
                                    random_state = 1234)

# Create  random forest models using the transformed data. 
rfr_C1 = RandomForestRegressorModel(Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# With Hyperparameters
RandomForestRegressorModel_WithParams(750, 10, Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# For Gradient Boosting 
gbr_C1 = GradientBoostingRegressorModel(Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# With Hyperparameters
GradientBoostingRegressorModel_WithParams(500, 10, Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# For Neural Network
# Neural Network models using the transformed data. 
NeuralNetworkRegressorModel(Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)

# Neural Network models with Optimized Parameters Values and transformed data. 
rSquare = NeuralNetworkModel_3Layer(50, 50, 20,
            'relu', Xp_test_C1, yp_test_C1, Xp_train_C1, yp_train_C1)     

